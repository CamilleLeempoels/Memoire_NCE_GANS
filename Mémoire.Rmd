---
title: "Noise-contrastive estimation of normalising constants and GANs"
header-includes:
  - \usepackage{mathrsfs}
  - \usepackage{bm}
  - \usepackage{dsfont}
  - \usepackage{amsmath}
  - \usepackage{mdframed}
  - \usepackage{placeins}
output: 
  pdf_document:
    number_sections: true
---

\tableofcontents

\newpage
\section{Fonctions génériques}

```{r include=FALSE}
library(ggplot2)
library(reshape2)
library(matrixStats)
library(knitr)
library(moments) 
library(tseries)
library(readr)
```

```{r}
# Palettes

pal5 = c("#3B9AB2", "#78B7C5", "#EBCC2A", "#E1AF00", "#DC863B")
pal4 = c("#3B9AB2", "#78B7C5", "#EBCC2A", "#DC863B")
pal2 = c("#3B9AB2", "#DC863B")
pal3 = c("#3B9AB2", "#EBCC2A", "#DC863B")

```


\subsection{Algorithme d'Hasting}

Utilité : simuler selon $p_m(.,\psi)$ pour un paramètre $\psi$ choisi. 

\begin{table}[!h]
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Argument} & \textbf{Type} & \textbf{Exemple} & \textbf{Indication} \\ \hline
x & vecteur & rcauchy(100, 0, 1) &  notre échantillon de densité inconnue \\ \hline
n & entier & 100 & taille de la simulation \\ \hline
psi & vecteur & c(0,1) & paramètres de la fonction h \\ \hline
h & fonction &  & fonction qui retourne $\overline{p_m}(.,\psi)$\\ \hline
\end{tabular}
\end{table}

```{r hasting}
hasting = function(x, n, psi, h){
  y = c()
  y = append(y, sample(x, 1))
  for (i in 2:n){
    y_ = rnorm(1, y[i-1], 1)
    u = runif(1)
    if ( u <= 
         (h(y_,psi) * dnorm(y_, y[i-1], 1))
         /(h(y[i-1],psi) * dnorm(y[i-1], y_, 1))
    ){ 
      y = append(y, y_)} 
    else { 
      y = append(y, y[i-1])
    }
  }
  return (y)
}
```

Ci-dessous une autre version qui génère un échantillon iid.

\begin{table}[!h]
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Argument} & \textbf{Type} & \textbf{Exemple} & \textbf{Indication} \\ \hline
x & vecteur & rcauchy(100, 0, 1) &  notre échantillon de densité inconnue \\ \hline
n & entier & 100 & taille de la simulation \\ \hline
psi & vecteur & c(0,1) & paramètres de la fonction h \\ \hline
h & fonction &  & fonction qui retourne $\overline{p_m}(.,\psi)$\\ \hline
$\epsilon$ & Entier & 2 & pas de décorrélation $\overline{p_m}(.,\psi)$\\ \hline
\end{tabular}
\end{table}

```{r hasting iid}
hasting_iid = function(x, n, psi, h, eps){
  y = c()
  y = append(y, sample(x, 1))
  for (i in 2:(n*eps)){
    y_ = rnorm(1, y[i-1], 1)
    u = runif(1)
    if ( u <= 
         (h(y_,psi) * dnorm(y_, y[i-1], 1))
         /(h(y[i-1],psi) * dnorm(y[i-1], y_, 1))
    ){ 
      y = append(y, y_)} 
    else { 
      y = append(y, y[i-1])
    }
  }
  filter = y * rep(c(1,rep(0, eps-1)), n)
  return (filter[filter != 0])
}
```

\subsection{MC MLE (Geyer)}

Utilité : retourne une estimation des paramètres selon la méthode décrite dans le papier de Geyer. 

```{r mcmle}
mc_mle = function(x, n, psi, h){
  
  m = length(x)

  y = hasting(x, n, psi, h)
    
  L = function(theta){
    return(sum(log(h(x,theta)/h(x,psi))) - m*log(mean(h(y,theta)/h(y,psi))))
  }
  
  theta = optim(
    par = rep(1,length(psi)),
    gr = "CG",
    control = list(fnscale=-1),
    fn = L
  )$par

  return(theta)
}
```


\subsection{NCE (Gutmann)}

Utilité : Retourne l'estimation de la constante et des paramètres.

\begin{table}[!h]
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Argument} & \textbf{Type} & \textbf{Exemple} & \textbf{Indication} \\ \hline
x & vecteur & rcauchy(100, 0, 1) & notre échantillon de densité inconnue \\ \hline
law\_y & fonction & rnorm & fonction qui retourne un échantillon suivant la loi $p_n$ \\ \hline
n & entier & 100 & taille de l'échantillon de bruit suivant la loi $p_n$ \\ \hline
params\_y & vecteur & c(0,1) & arguments de la fonction law\_y \\ \hline
log\_pm & fonction &  & fonction qui retourne le logarithme de la densité $p_m$ \\ \hline
log\_pn & fonction &  & fonction qui retourne le logarithme de la densité $p_n$ \\ \hline
size\_theta & entier & 3 & taille de $\theta$, vaut habituellement 2 ou 3 \\ \hline
\end{tabular}
\end{table}

```{r nce}
nce = function(x, law_y, params_y, log_pm, log_pn, size_theta, n){
  
  y = do.call(law_y, c(list(n),params_y))
  
  m = length(x)
  
  h = function(u, theta){
    return( 1 / (1 + n/m * exp(log_pn(u) - log_pm(u, theta))))
  }
  
  J = function(theta){
    return( sum(log(h(x, theta))) + sum(log(1 - h(y, theta))) )
  }
  
  theta = optim(
    par = rep(1, size_theta),
    gr = "CG",
    control = list(fnscale=-1),
    fn = J
  )$par
  
  return(c(theta[-size_theta], exp(-theta[size_theta])))
}

nce(x, sim_multivar_dis_unif, list(nb_spins,-1,1), log_pm_ising, log_pn_multivar_dis_unif, 2, n)

```

\subsection{Graphiques}

Utilité : afficher l'histogramme pour un échantillon de données $x$.

```{r print_hist}
print_hist = function(x) {
  df = data.frame(x = x)
  hist_x = ggplot(df, aes(x=x)) + 
           geom_histogram(aes(y = stat(count)/sum(count)), bins = 20, color="white") + 
           theme(aspect.ratio = 1) + 
           labs(y = "Fréquence") + 
           ggtitle("Distribution de l'échantillon x")
  print(hist_x)
}
```

Utilité : pour NCE, afficher l'évolution des paramètres au fur et à mesure de l'augmentation de n (la dimension de l'échantillon de bruit)

```{r NCE_evol_params}
NCE_evol_params = function(x, law_y, params_y, log_pm, log_pn, size_theta, ratio, steps, labels) {
  
  # Creation de l'abscisse
  m = length(x)
  N = seq(0, m*ratio, length.out = steps + 1)
  
  # Creation de l'ordonnée
  theta = c()
  for (n in N) {
    theta = append(theta, nce(x, law_y, params_y, log_pm, log_pn, size_theta, n))
  }
  
  # Formatage des données
  theta = t(rbind(matrix(theta, nrow = size_theta),N))
  df = as.data.frame(theta)
  df_melted = melt(df, id.vars = "N")
  
  # Plot
  plot_df = ggplot(df_melted, aes(x = N, y = value)) + 
  geom_line(aes(color = variable, group = variable)) +
  geom_point(aes(color = variable, group = variable)) +
  labs(title = "Evolution des paramètres par rapport au bruit", 
       x = "n (taille du bruit)", 
       y = "Paramètres", 
       color = "Légende") +
  scale_color_manual(labels = labels, values = c("blue", "red", "orange"))
  
  print(plot_df)
  
  #return(theta)
}
```

Note : il faudrait optimiser le temps de calcul de ces fonctions, peut-être en matriciel au lieu des boucles ou bien avec du calcul en parralèle sur CPU/GPU


\newpage

\section{Applications}
\subsection{Exemple basique : la loi normale}

Soit $x$ l'échantillon de taille $m$ obtenu selon la loi de densité inconnue $p_d$.

On considère ici que $p_d$ appartient à la famille de fonctions paramétrées par $\theta = (c, \mu, \sigma)$ suivante :

$$p_m(u;\theta) = \frac{1}{Z(\mu, \sigma)} \times exp \big[-\frac{1}{2} \big(\frac{u - \mu}{\sigma} \big)^2 \big]  \quad \mbox{d'où} \quad ln(p_m(u;\theta)) = c - \frac{1}{2} \big(\frac{u}{\sigma} - \frac{\mu}{\sigma} \big)^2$$
```{r exemple1}
pm_barre = function(u, theta){
  return(exp(-0.5 * ((u - theta[1]) / theta[2]) ** 2))
}

log_pm = function(u,theta){
  return(theta[3] - 1/2 * (u/theta[2] - theta[1]/theta[2]) ** 2)
  # theta[1] = mu / theta[2] = sigma / theta[3] = c
}

log_pn_cauchy = function(u){
  return(log(dcauchy(u, mean(x), sd(x))))
}

m = 10000
n = 10000
x = rnorm(m, 2, 4)
size_theta = 3
```

```{r mc_mle application}
# METHODE MC MLE
mc_mle(x, n, c(mean(x),sd(x)), pm_barre)
```

Etudions l'impact de la dimension des échantillons sur la convergence des estimateurs.

```{r csv mc mle, eval=FALSE, include=FALSE}
#NE PAS FAIRE TOURNER CETTE CELLULE, HYPER LONG, IMPORTER LE CSV A LA PLACE

df_mcmle = data.frame(matrix(ncol = 4, nrow = 0))
colnames(df_mcmle_2) = c("param_1", "param_2", "size_data", "ratio_noise_data")

M = c(1000, 10000)


for (m in M){
  x = rnorm(m, 2, 4)
  psi = c(mean(x), sd(x))
  N = c(1, 10)
  for (n in N) {
    for (i in 1:100) {
       df_mcmle_2[nrow(df_mcmle_2) + 1, ] = c(mc_mle(x, m*n, psi, pm_barre), m, n)
    }
  }
}
```

```{r processing csv mc mle, include=FALSE}
df_mcmle <- read_csv("df_mcmle.csv")[,-1]

df_mcmle$size_data = as.factor(df_mcmle$size_data)
df_mcmle$ratio_noise_data = as.factor(df_mcmle$ratio_noise_data)

df_mcmle_filt = subset(df_mcmle, param_2 <= 6 & param_2 >= 0 & param_1 >= -2 & param_1 <= 6)
```

```{r graph mc mle cv}
ggplot(df_mcmle_filt, aes(x = param_1, y = param_2, color = size_data, shape = ratio_noise_data)) + geom_point() + scale_color_manual(values = pal2)
```

Etudions l'impact du choix de $\psi$ sur la convergence des estimateurs.

```{r csv mc mle psi, eval=FALSE, include=FALSE}
df_mcmle_psi = data.frame(matrix(ncol = 3, nrow = 0))
colnames(df_mcmle_psi) = c("param_1", "param_2", "ratio_alpha_psi")

x = rnorm(10000, 2, 4)
for (r in c(1,3,5,7)){
  for (i in 1:100) {
     df_mcmle_psi[nrow(df_mcmle_psi) + 1, ] = c(mc_mle(x, 10000, c(2*r, 4*r), pm_barre), r)
  }
}
write.csv(df_mcmle_psi, "df_mcmle_psi.csv")

```


```{r graph mc mle psi, message = FALSE}
df_mcmle_psi <- read_csv("df_mcmle_psi.csv")[,-1]

df_mcmle_psi = df_mcmle_psi[order(-df_mcmle_psi$ratio_alpha_psi),]

df_mcmle_psi$ratio_alpha_psi = as.factor(df_mcmle_psi$ratio_alpha_psi)
ggplot(df_mcmle_psi, aes(x = param_1, y = param_2, color = ratio_alpha_psi)) + geom_point() + scale_color_manual(values = pal5)
```


```{r nce application}
# METHODE NCE
nce(x, rcauchy, c(mean(x),sd(x)), log_pm, log_pn_cauchy, size_theta, n)
```

```{r csv nce, eval=FALSE, include=FALSE}
#NE PAS FAIRE TOURNER CETTE CELLULE, HYPER LONG, IMPORTER LE CSV A LA PLACE

df_nce = data.frame(matrix(ncol = 5, nrow = 0))
colnames(df_nce) = c("param_1", "param_2", "const", "size_data", "ratio_noise_data")

M = c(1000, 10000, 100000)


for (m in M){
  x = rnorm(m, 2, 4)
  psi = c(mean(x), sd(x))
  N = c(1, 10)
  for (n in N) {
    for (i in 1:50) {
       df_nce[nrow(df_nce) + 1, ] = c(nce(x, rcauchy, psi, log_pm, log_pn_cauchy, size_theta, m*n), m, n)
    }
  }
}
write.csv(df_nce, "df_nce.csv")
```

```{r process csv nce, include=FALSE}
df_nce <- read_csv("df_nce.csv")[,-1]

df_nce$size_data = as.factor(df_nce$size_data)
df_nce$ratio_noise_data = as.factor(df_nce$ratio_noise_data)

df_nce$const_error = abs(df_nce$const - 4*sqrt(2*pi))
```

```{r graph nce params}
ggplot(df_nce, aes(x = param_1, y = param_2, color = size_data, shape = ratio_noise_data)) + geom_point() + scale_color_manual(values = pal3)
```

```{r graph nce const}
ggplot(df_nce, aes(x = size_data, y = const_error, color = ratio_noise_data)) + geom_boxplot() + scale_color_manual(values = pal2)
```

\subsection{Modèle d'Ising}

```{r}
# temps estimé 1min n=500, iter=100k
library('isingLenzMC')

sim_ising_1D = function(beta, nb_spins, nb_config){
  config  = rep(1,n) 
  data = matrix(nrow = iter, ncol = n)
  data[1, ] = config
  for (k in 2:iter){
    data[k,] = isStep1D(beta, config, 1.0, 0.0, 1)$vec # tirage avec Metropolis
  }
  return(data)
}

beta    = 0.9             # paramètre de temperature 
n       = 500             # nombre de sites
config  = rep(1,n)        # generer une configuration à n sites
config1 = genConfig1D(n)  # generer une configuration à n sites aléatoirement

energie = c()
iter = 10000
data = matrix(nrow= iter, ncol = n)

for (k in 1:iter){
  # on tire successivement des configurations jusqu'à obtenir convergence du niveau d'énergie
  #ce que l'on observera graphiquement
  config = isStep1D(beta, config, 1.0, 0.0, 1)$vec # tirage avec Metropolis
  data[k,] = config
  energie   = c(energie,totalEnergy1D(config,1.0,0.0))
}
plot(energie, type ='l')
```


On définit la fonction $coeur$ qui est à une constante de normalisation près la mesure de Gibbs associée à au modèle d'Ising de paramètre $\beta$.
On définit la fonction $ising1D(n)$ qui est rend toutes les configurations de spins possibles d'un modèle d'Ising unidimensionnel à $n$ sites. 

```{r}
coeur = function(constante_normalisation,beta=0.9,config){
  
  # constante_normalisation :  fonction de partition inconnue
  # beta                    :  paramètre de température 
  # config                  :  une configuration de spins
  
  # Lorsque 'constante_normalisation' == vraie valeur de la fonction de partition pour le paramètre beta 
  #       alors return la probabilité de la configuration de spins 'config'
  
  return ( exp(-beta*totalEnergy1D(config,1.0,0.0))/constante_normalisation )
}
  
ising1D = function(n){

  # n : nombre de sites pour une configuration
  
  # return l'ensemble des configuration 1D possibles dans une matrice
  # chaque ligne correspond à une configuration possible
  
  if (n==1)
    {
    return(matrix(c(1,-1),nrow=2))
    }
  else
    {
    return(cbind( rbind(ising1D(n-1),ising1D(n-1)), c(rep(1,2**(n-1)),rep(-1,2**(n-1)))  ))
    }
}

```

```{r eval=FALSE}
# un petit test de ce qu'on fait

n       = 15   # nombre de sites 
config  = ising1D(n) # ensemble des configurations 1D à n sites
beta    = 0.9 # paramètre de température
energie = c()

for (k in 1:2**n){ energie = c(energie,totalEnergy1D(c(config[k,]), 1, 0) ) } # totalEnergy1D(config[k,], 1, 0), 1 interraction voisins, 0 champs ext

cst_normalisation  = sum(exp(-beta*energie))

proba  = 0
for (k in 1:2**n){ proba = proba + coeur(cst_normalisation,beta=0.9,config[k,]) }
(proba)
print("if proba = 1, then it is good")
```
Il se peut qu'il y ait conflit de notations pour cette sous section d'Ising avec les notations du précédent exemple.
De plus, il y'a une erreur en sortie que je n'arrive pas résoudre pour l'instant. Mais je ne pense pas qu'elle soit très compliquée...

```{r eval=FALSE}
log_pn = function(u,param_pn){
  # pn densite iid N(param_pn[1],param_pn[2]) de dim length(u) 
  # Verifier que c'est bien ce qui est codé
  n     = length(u)
  log_d = 0 
  for (k in 1:n){ log_d = log_d - 0.5 * ((u[k] - param_pn[1]) / param_pn[2]) ** 2 }
  return(-0.5*n*log(2*pi*(param_pn[2]**2)) + log_d)
}

log_pm = function(configuration,beta,theta){
  # theta = -log(Z)
  # interraction entre sites = 1 et avec le champs extérieure = 0
  return ( -beta*totalEnergy1D(configuration,1,0) + theta )
}


nce_ising = function(matrix_ising,beta,log_pm,theta,law_pn,param_pn,log_pn,b_size){
  # matrix_ising = matrice ou en ligne sont rangées les observations d'Ising en 1D
  # beta         = paramètre de température de modèle d'Ising \in ]0,1[
  # log_pm       = logarithme de la densité d'Ising
  # theta        = paramètre à optimiser , en lien avec la constante de normalisation
  
  # law_pn   = loi du bruit qui doit contenir dans son support {0,1}
  # param_pn = paramètres de la loi law_pn
  # log_pn   = log de la densité de pn
  
  # b_size  = nombre d'observation du bruit, à ne pas confondre avec la taille d'une observation du bruit qui est fixée à n
  
  n     = dim(matrix_ising)[2] # n = nombre de sites pour les observations d'Ising en 1D
  m     = dim(matrix_ising)[1] # m = nombre d'observations d'Ising en 1D
  
  bruit = matrix(do.call(law_pn,c(list(b_size*n),param_pn)),ncol=n) # échantillon de vecteurs de bruit disposé en matrice, ligne par ligne
  
  h = function(configuration,theta){return( 1 / (1 + b_size/m * exp(log_pn(configuration,param_pn) - log_pm(configuration,beta,theta))))}
  
  J = function(theta){
    # x vecteur de densite inconnue 
    # y vecyeur de bruit 
    objectif = 0
    for (k in 1:m){objectif = objectif + log(h(matrix_ising[k,],theta)) }   # composante de l'objectif liée aux données
    for (k in 1:b_size){objectif = objectif + log(1 - h(bruit[k,],theta)) } # composante de l'objectif liée au bruit
    return( objectif )
    }
  
  solution = optimize(f = J,
                      interval = c(-1e5,1e5),
                      maximum  = FALSE)
  
  return(exp(-solution))
}


n            = 10
matrix_ising = ising1D(n)[100:900,]
beta         = 0.9
theta        = 1
law_pn       = rnorm
param_pn     = c(0,1)
b_size       = 2**n

nce_ising(matrix_ising,beta,log_pm,theta,law_pn,param_pn,log_pn,b_size)
```


```{r}
# --------------- Ising ---------------------------------------------------------------------------------------------------------

all_config_ising1D = function(nb_spins){
  # n : nombre de sites pour une configuration
  # return l'ensemble des configuration 1D possibles dans une matrice
  # chaque ligne correspond à une configuration possible
  
  if (n==1) return(matrix(c(1,-1),nrow=2))
  return(cbind( rbind(ising1D(n-1),ising1D(n-1)), c(rep(1,2**(n-1)),rep(-1,2**(n-1)))  ))
}

sim_ising_1D = function(beta, nb_config, nb_spins){
  config  = rep(1,nb_spins) 
  data = matrix(nrow = nb_config, ncol = nb_spins)
  data[1, ] = config
  for (k in 2:nb_config){
    data[k,] = isStep1D(beta, data[k-1,], 1.0, 0.0, 1)$vec # tirage avec Metropolis
  }
  return(data)
}

calc_energie = function(sim_ising, nb_config){
  sim_ising_split = split(sim_ising, as.factor(1:nb_config))
  energie = sapply(sim_ising_split, totalEnergy1D, 1, 0)   # interaction entre sites = 1 et avec le champ extérieur = 0
  return(energie)
}

calc_const = function(beta,nb_spins){
  ens_config = all_config_ising1D(nb_spins) # ensemble des configurations 1D à nb_spins sites
  energie_total = calc_energie(ens_config, 2**nb_spins)
  return(sum(exp(-beta*energie_total)))
}

# --------------- Simulations d'échantillons ------------------------------------------------------------------------------------

sim_multivar_dis_unif = function(nb_samples, size_sample, a, b){
  return(matrix(sample(c(a,b), nb_samples * size_sample, replace=T), ncol = size_sample))
}

sim_multivar_cont_unif = function(nb_samples, size_sample, minimum, maximum){
  return(matrix(runif(nb_samples * size_sample, minimum, maximum), ncol = size_sample))
}


# --------------- Fonctions pm_barre, pm et pn -----------------------------------------------------------------------------------

log_pm_ising = function(u,theta){
  # theta[1] = beta
  # theta[2] = -log(Z)
  return ( - theta[1] * calc_energie(u, dim(u)[1]) + theta[2] )
}

log_pn_multivar_cont_unif = function(u){
  return(log(rowProds(dunif(u, -1, 1))))
}

log_pn_multivar_dis_unif = function(u){
  return(log(rowProds(ifelse(u == -1, 1/2, ifelse(u == 1, 1, 0)))))
}

pn_multivar_dis_unif = function(u, psi){
  return(rowProds(ifelse(u == -1, 1/2, ifelse(u == 1, 1, 0))))
}

pn_multivar_cont_unif = function(u, psi){
  return(rowProds(dunif(u, -1, 1)))
}

pm_barre_ising = function(u, theta){
  return(exp(-theta[1] * calc_energie(u, dim(u)[1])))
}

```


```{r}

# ------------- Variables globales -----------------------------------------------------------------------------------------------

m = iter
n = iter
beta_alpha = 0.9
beta_psi = 0.5
nb_spins = 20
nb_config = 1000
x = sim_ising_1D(beta_alpha, nb_config, nb_spins)
y = sim_ising_1D(beta_psi, nb_config, nb_spins)

energie_x = calc_energie(x, nb_config)
energie_y = calc_energie(y, nb_config)

# --------------- Calcul numérique de la constante --------------------------------------------------------------------------

calc_const(beta_alpha, nb_spins)

# --------------- Estimation avec MC MLE ------------------------------------------------------------------------------------


#L = function(theta){ return(sum(log(pm_barre_ising(x,theta)/pm_barre_ising(x,psi))) - m*log(mean(pm_barre_ising(y,theta)/pm_barre_ising(y,psi))))}

#theta = optim( par = c(0.9, 1), gr = 'CG', control = list(fnscale=-1), fn = L)$par


# --------------- Estimation avec la regression logistique inverse ------------------------------------------------------

rev_log_reg_with_noise(x, sim_multivar_cont_unif, n, c(beta_alpha), list(nb_spins, -1, 1), pm_barre_ising, pn_multivar_cont_unif)


# --------------- Estimation avec NCE -----------------------------------------------------------------------------------

#nce(x, sim_multivar_dis_unif, list(nb_spins,-1,1), log_pm_ising, log_pn_multivar_dis_unif, 2, n)

```

\newpage
\section{Nouvelles approches}
\subsection{Bootstrap}


```{r NCE_bootstrap}

# Calcul de {size_boot} estimateurs par bootstrap
NCE_bootstrap = function(x, law_y, params_y, log_pm, log_pn, size_theta, n, size_boot, labels) {
  m = length(x)
  theta_bootstrap = c()
  x_bootstrap = x
  for (i in 1:size_boot) {
    theta_bootstrap = append(theta_bootstrap, nce(x_bootstrap, 
                                                  law_y, 
                                                  params_y, 
                                                  log_pm, 
                                                  log_pn, 
                                                  size_theta, 
                                                  n))
    x_bootstrap = sample(x, size = m, replace=TRUE)
  }
  return(matrix(theta_bootstrap, nrow = size_theta))
}

# Plot la moyenne empirique des estimateurs bootstrap en fonction du nombre d'estimateurs
NCE_bootstrap_plot = function(matrix_theta_bootstrap) {
  
  # Formatage des données pour plot
  array_boot = 1:length(matrix_theta_bootstrap[1,])
  df = as.data.frame(cbind(t(rowCumsums(matrix_theta_bootstrap))/array_boot,array_boot))
  df_melted = melt(df, id.vars = "array_boot")
  
  # Plot
  plot_df = ggplot(df_melted, aes(x = array_boot, y = value)) + 
  geom_line(aes(color = variable, group = variable)) +
  labs(title = "Evolution des paramètres par bootstrap", 
       x = "Taille du bootstrap", 
       y = "Paramètres", 
       color = "Légende") +
  scale_color_manual(labels = labels, values = c("blue", "red", "orange"))
  print(plot_df)
}


# etude bootstrap de l'estimateur
bootstrap = function(matrix, alpha){
  return(data.frame(
    theta = matrix_theta_bootstrap[,1],
    biais = rowMeans(matrix_theta_bootstrap) - matrix_theta_bootstrap[,1],
    IC = rowQuantiles(matrix_theta_bootstrap, probs = c(alpha/2, 1-alpha/2))
  ))
}

```

```{r test_NCE_bootstrap}
x_test = rnorm(1000,2,4)

matrix_theta_bootstrap = NCE_bootstrap(x_test, rcauchy, c(mean(x_test),sd(x_test)), log_pm, log_pn_cauchy, 3, 1000, 10, c("mu", "sigma", "exp(-c)"))

kable(bootstrap(matrix_theta_bootstrap, 0.05))
```



\subsection{Récursivité}

Utilité : améliorer récursivement la précision de l'estimation via les estimations précédentes

```{r mc_mle_recursif, eval=FALSE, include=FALSE}
m = 10000
n = 10000
x = rnorm(m, 2, 4)
psi = c(mean(x),sd(x))

df_recurs_naif = data.frame(matrix(ncol = 3, nrow = 0))
colnames(df_recurs_naif) = c("iteration", "param_1", "param_2")

for (i in 1:100) {
  y = hasting(x, n, psi, pm_barre)
  df_recurs_naif[nrow(df_recurs_naif) + 1, ] = c(i, psi)

  L = function(theta){
    return(sum(log(pm_barre(x,theta)/pm_barre(x,psi))) - m*log(mean(pm_barre(y,theta)/pm_barre(y,psi))))
  }
  
  psi = optim(
    par = rep(1,length(psi)),
    gr = "CG",
    control = list(fnscale=-1),
    fn = L
  )$par
}

write.csv(df_recurs_naif, "df_recurs_naif.csv")
```

```{r graph point mc mle recur, message = FALSE}
df_recurs_naif <- read_csv("df_recurs_naif.csv")[,-1]
ggplot(df_recurs_naif, aes(x = param_1, y = param_2, color = iteration)) + geom_point()
```

```{r graph lines mc mle recur, message = FALSE}
df_recurs_naif <- read_csv("df_recurs_naif.csv")[,-1]
colnames(df_recurs_naif) = c("iteration", "param_1", "param_2")

df_recurs_naif_melted = melt(df_recurs_naif, id.vars = "iteration")
ggplot(df_recurs_naif_melted, aes(x = iteration, y = value)) + geom_line(aes(color = variable, group = variable)) + scale_color_manual(values = pal2)
```
Cette approche naïve n'améliore pas la précision de notre estimation. 

Nouvelle approche à venir.


```{r mc_mle_recursif_3, eval=FALSE, include=FALSE}
#NE FONCTIONNE PAS ENCORE

mc_mle_recursif_3 = function(x, n, psi, h, size_of_loop){
  
  m = length(x)
  PSI = data.frame(matrix(ncol = 2, nrow = 0))
  colnames(PSI) = c("param_1", "param_2")
  denom_x = 1
  denom_y = 1
  Y = c(hasting(x, n, psi, h))
  PSI[nrow(PSI) + 1, ] = psi
  print(PSI)

  for (i in 1:size_of_loop) {
    print(PSI)
    print(denom_x)
    print(denom_y)

    L = function(theta){
      return(sum(log(denom_x * h(x,theta)/h(x,PSI[i,]))) - m*log(mean(denom_y * h(Y,theta)/h(Y,PSI[i,]))))
      #return(sum(log(h(x,theta)/h(x,psi))) - m*log(mean(h(y,theta)/h(y,psi))))
    }
    
    PSI[nrow(PSI) + 1, ] = optim(
      par = rep(1,length(psi)),
      gr = "CG",
      control = list(fnscale=-1),
      fn = L
    )$par
    
    Y = append(Y, hasting(x, n, PSI[i,], h))
    
    denom_x = 0
    for (k in 1:i) {denom_x = denom_x + h(x, PSI[k,])}
    denom_y = 0
    for (k in 1:i) {denom_y = denom_y + h(Y, PSI[k,])}
  }

  return(PSI)
}

mc_mle_recursif_3(x, n, psi, pm_barre, 10)
```

\subsection{Hasting iid}

Afin de pouvoir appliquer numériquement la méthode de Reverse logistic regression, on aurait besoin de savoir simuler un échantillon iid suivant une loi dont on ne connait pas la constante de normalisation. L'idée est d'utiliser l'algorithme d'Hasting et de ne conserver qu'un échantillon tous les $\epsilon$ pas. Le code est en haut de ce document. 

Etude de l'impact du choix du pas.

```{r eval=FALSE, include=FALSE}
df_hasting_iid = data.frame(matrix(ncol=7, nrow=0))
colnames(df_hasting_iid) = c("epsilon", "mean", "sd", "skew", "kurtosis", "time", "mean_autocor")
init = rnorm(1, 0, 1)
n = 10000

for (eps in 1:10){
  for (i in 1:20) 
  { 
    t1<-Sys.time()
    y = hasting_iid(init, n, c(0,1), pm_barre, eps)
    t2<-Sys.time()
    t = difftime(t2, t1)
    df_hasting_iid[nrow(df_hasting_iid) + 1, ] = c(eps, mean(y), sd(y), skewness(y), kurtosis(y), t, mean(acf(y, plot = FALSE)$acf))
  }
}

df_hasting_iid$mean = abs(df_hasting_iid$mean)
df_hasting_iid$sd = abs(df_hasting_iid$sd - 1)
df_hasting_iid$skew = abs(df_hasting_iid$skew)
df_hasting_iid$kurtosis = abs(df_hasting_iid$kurtosis - 3)

write.csv(df_hasting_iid, "df_hasting_iid.csv")
```

```{r include=FALSE}
df_hasting_iid <- read_csv("df_hasting_iid.csv")[,-1]

df_hasting_iid_agg = aggregate(
  cbind(
    df_hasting_iid$mean, 
    df_hasting_iid$sd,
    df_hasting_iid$skew,
    df_hasting_iid$kurtosis,
    df_hasting_iid$time,
    df_hasting_iid$mean_autocor
  ),
  list(df_hasting_iid$epsilon),
  mean
)

colnames(df_hasting_iid_agg) = c("epsilon", "mean", "sd", "skew", "kurtosis", "time", "mean_autocor")

df_hasting_iid_melted = melt(df_hasting_iid_agg[,-6], id.vars = "epsilon")
```

```{r}
ggplot(df_hasting_iid_melted, aes(x = epsilon, y = value)) + geom_line(aes(color = variable, group = variable)) + scale_color_manual(values = pal5)

#ggplot(df_hasting_iid, aes(x = epsilon, y = time)) + geom_line()
```
$\epsilon = 2$ semble être un bon choix au regard du gain en terme d'auto-corrélation et du temps de calcul.

\subsection{Reverse logistic regression : deux lois de même famille}

La maximisation de la fonction objectif $$l_n(\eta) = \sum_{j=1}^m \sum_{i=1}^{n_j} log(p_j(X_{i,j}, \eta))$$ permet d'estimer les $\eta$ (qui sont fonction des constantes de normalisation des $h_j$). On utilise les notations suivantes :

$$\eta_j = -log(Z_j) + log(\frac{n_j}{n}) ~\mbox{ avec } Z_j \mbox{ la constante de normalisation de } h_j$$
$$p_j(x) = \frac{h_j(x)e^{\eta_j}}{\sum_{k=1}^m h_k(x)e^{\eta_k}}$$
Exemple avec $m=2$, $n = n_1 + n_2 = 1000 + 1000$, et pour coller avec les méthodes différentes on va prendre $h_1$ la densité non normalisée d'une $\mathscr{N}(\alpha)$ dont on a estimé $\alpha$ par MC MLE et $h_2$ la densité non normalisée d'une $\mathscr{N}(\psi)$ avec $\psi$ qu'on choisit. 

```{r rev_log_reg}
rev_log_reg = function(x, alpha, n, psi, h, eps){
  
  m = length(x)
  y = hasting_iid(x, n, psi, h, eps)
  
  # calcul des probabilités p_j
  denom = function(sample, eta) {
    return(pm_barre(sample, alpha)*exp(eta[1]) + pm_barre(sample,psi)*exp(eta[2]))}
  p_1 = function(sample, eta){
    return (pm_barre(sample, alpha)*exp(eta[1]) / denom(sample, eta))}
  p_2 = function(sample, eta){
    return (pm_barre(sample, psi)*exp(eta[2]) / denom(sample, eta))}
  
  # fonction objectif
  L = function(eta) {
    return(sum(log(p_1(x, eta))) + sum(log(p_2(y, eta))))}
  
  # initialisation descente de gradient
  eta1 = -log(sd(x)*sqrt(2*pi)) + log(m/(m+n))
  eta2 = -log(sd(y)*sqrt(2*pi)) + log(n/(m+n))
  
  # optimisation
  const = optim(
    par = c(eta1,eta2),
    gr = "CG",
    control = list(fnscale=-1),
    fn = L
  )$par
  
  a = exp(-const[1] + log(m/(m+n)))
  return(a)
}
```

```{r rev_log_reg init}
pm_barre = function(u, theta){
  return(exp(-0.5 * ((u - theta[1]) / theta[2]) ** 2))
}

m = 1000
n = 10000
x = rnorm(m,2,4)
psi = c(mean(x), sd(x))

alpha = mc_mle(x, n, psi, pm_barre)
print(alpha)
```

```{r rev_log_reg ex1}
print(rev_log_reg(x, alpha, n, c(8,8), pm_barre, 2))
```

Etude de l'impact de la dimension, du ratio et des paramètres sur la convergence de la constante.

```{r eval=FALSE, include=FALSE}
df_rev_log_reg = data.frame(matrix(ncol = 4, nrow = 0))
colnames(df_rev_log_reg) = c("const", "size_data", "ratio_noise_data", "ratio_alpha_psi")

M = c(1000, 5000, 10000, 15000)
N = c(1, 10)
alpha = c(2,4)

for (m in M){
  x = rnorm(m, 2, 4)
  for (n in N) {
    for (r in 1:4) {
      for (i in 1:15) {
         df_rev_log_reg[nrow(df_rev_log_reg) + 1, ] = c(rev_log_reg(x, alpha, n*m, alpha*r, pm_barre, 2), m, n, r)
      }
    }
  }
}

write.csv(df_rev_log_reg, "df_rev_log_reg.csv")
```

```{r message=FALSE}
df_rev_log_reg <- read_csv("df_rev_log_reg.csv")[,-1]

df_rev_log_reg_agg = aggregate(const ~ size_data + ratio_noise_data + ratio_alpha_psi,
                    data = df_rev_log_reg,
                    FUN = mean)
df_rev_log_reg_agg$ratio_alpha_psi = as.factor(df_rev_log_reg_agg$ratio_alpha_psi)
df_rev_log_reg_agg$ratio_noise_data = as.factor(df_rev_log_reg_agg$ratio_noise_data)
df_rev_log_reg_agg$size_data = as.numeric(df_rev_log_reg_agg$size_data)
df_rev_log_reg_agg$const_error = abs(df_rev_log_reg_agg$const - 4*sqrt(2*pi))

ggplot(df_rev_log_reg_agg, aes(x = size_data, y = const_error, color = ratio_alpha_psi, shape = ratio_noise_data)) + geom_line() + geom_point() + scale_color_manual(values = pal4)
```


\subsection{Reverse logistic regression : deux lois de familles différentes}

On reprend les notations ci-dessus (notations du papier). Exemple avec $m=2$, $n = n_1 + n_2$, et pour coller avec les méthodes différentes on va prendre $h_1$ la densité non normalisée d'une $\mathscr{N}(\alpha)$ dont on a estimé $\alpha$ par MC MLE et $h_2$ la densité d'une loi usuelle, on en connait donc la constante de normalisation. On note $\psi$ le paramètre de cette loi usuelle. 
```{r}
rev_log_reg_with_noise = function(x, law_noise, n, alpha, psi, h1, h2){
  
  y = do.call(law_noise, c(list(n),psi))
  m = length(x)
  
  # calcul des probabilités p_j
  denom = function(sample, eta) {
    return(h1(sample, alpha)*exp(eta[1]) + h2(sample,psi)*exp(eta[2]))}
  p_1 = function(sample, eta){
    return (h1(sample, alpha)*exp(eta[1]) / denom(sample, eta))}
  p_2 = function(sample, eta){
    return (h2(sample, psi)*exp(eta[2]) / denom(sample, eta))}
  
  # fonction objectif
  L = function(eta) {
    return(sum(log(p_1(x, eta))) + sum(log(p_2(y, eta))))}
  
  # initialisation descente de gradient
  eta1 = -log(sd(x)*sqrt(2*pi)) + log(m/(m+n))
  eta2 = -log(sd(y)*sqrt(2*pi)) + log(n/(m+n))
  
  # optimisation
  const = optim(
    par = c(eta1,eta2),
    gr = "CG",
    control = list(fnscale=-1),
    fn = L
  )$par
  
  b = exp(-const[2] + log(m/(m+n)))
  a = exp(-const[1] + log(m/(m+n)))
  
  # la constante de h_2 vaut normalement 1 si h_2 est une loi de densité, donc b est exactement le coefficent de proportionalité
  return(a/b*m/n)
}
```

```{r rev_log_reg_noise init}
pm_barre = function(u, theta){
  return(exp(-0.5 * ((u - theta[1]) / theta[2]) ** 2))
}

pn_cauchy = function(u, psi){
  return(dcauchy(u, psi[1], psi[2]))
}

m = 1000
n = 10000
x = rnorm(m,2,4)
psi = c(mean(x), sd(x))

alpha = mc_mle(x, n, psi, pm_barre)
print(alpha)
```
```{r}
rev_log_reg_with_noise(x, rcauchy, n, c(2,4), c(mean(x),sd(x)), pm_barre, pn_cauchy)
```

```{r eval=FALSE, include=FALSE}
df_rev_log_reg_noise = data.frame(matrix(ncol = 4, nrow = 0))
colnames(df_rev_log_reg_noise) = c("const", "size_data", "ratio_noise_data", "law_noise")

M = c(1000, 5000, 10000, 15000)
N = c(1, 10)
alpha = c(2,4)

for (m in M){
  x = rnorm(m, 2, 4)
  for (n in N) {
    for (i in 1:1000) {
       df_rev_log_reg_noise[nrow(df_rev_log_reg_noise) + 1, ] = c(rev_log_reg_with_noise(x, rcauchy, n*m, alpha, alpha, pm_barre, dcauchy), m, n, "cauchy")
    }
    for (i in 1:1000) {
       df_rev_log_reg_noise[nrow(df_rev_log_reg_noise) + 1, ] = c(rev_log_reg_with_noise(x, rnorm, n*m, alpha, alpha, pm_barre, dnorm), m, n, "normale")
    }
    for (i in 1:1000) {
       df_rev_log_reg_noise[nrow(df_rev_log_reg_noise) + 1, ] = c(rev_log_reg_with_noise(x, runif, n*m, alpha, c(min(x),max(x)), pm_barre, dunif), m, n, "unif")
    }
  }
}

write.csv(df_rev_log_reg_noise, "df_rev_log_reg_noise.csv")
```

```{r message = FALSE}
df_rev_log_reg_noise <- read_csv("df_rev_log_reg_noise.csv")[,-1]

df_rev_log_reg_agg = aggregate(const ~ size_data + ratio_noise_data + law_noise,
                    data = df_rev_log_reg_noise,
                    FUN = mean)
df_rev_log_reg_agg$ratio_noise_data = as.factor(df_rev_log_reg_agg$ratio_noise_data)
df_rev_log_reg_agg$size_data = as.numeric(df_rev_log_reg_agg$size_data)
df_rev_log_reg_agg$const_error = abs(df_rev_log_reg_agg$const - 4*sqrt(2*pi))

ggplot(df_rev_log_reg_agg, aes(x = size_data, y = const_error, color = law_noise, shape = ratio_noise_data)) + geom_line() + geom_point() + scale_color_manual(values = pal3)
```

