---
title: "Noise-contrastive estimation of normalising constants and GANs"
header-includes:
  - \usepackage{mathrsfs}
  - \usepackage{bm}
  - \usepackage{dsfont}
  - \usepackage{amsmath}
  - \usepackage{mdframed}
  - \usepackage{placeins}
output: pdf_document
fig_width: 3
fig_height: 2
---


\section{Fonctions génériques}

```{r}
library(ggplot2)
library(reshape)
library(matrixStats)
```

\subsection{Algorithme d'Hasting}

Utilité : simuler selon $p_m(.,\psi)$ pour un paramètre $\psi$ choisi. 

\begin{table}[!h]
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Argument} & \textbf{Type} & \textbf{Exemple} & \textbf{Indication} \\ \hline
x & vecteur & rcauchy(100, 0, 1) &  notre échantillon de densité inconnue \\ \hline
n & entier & 100 & taille de la simulation \\ \hline
psi & vecteur & c(0,1) & paramètres de la fonction h \\ \hline
h & fonction &  & fonction qui retourne $\overline{p_m}(.,\psi)$\\ \hline
\end{tabular}
\end{table}

```{r}
hasting = function(x, n, psi, h){
  y = c()
  y = append(y, sample(sample(x, 1)))
  for (i in 2:n){
    y_ = rnorm(1, y[i-1], 1)
    u = runif(1)
    if ( u <= 
         (h(y_,psi) * dnorm(y_, y[i-1], 1))
         /(h(y[i-1],psi) * dnorm(y[i-1], y_, 1))
    ){ 
      y = append(y, y_)} 
    else { 
      y = append(y, y[i-1])
    }
  }
  return (y)
}
```


Note : on peut très certainement écrire sous forme matricielle cette fonction pour une meilleure performance.

\subsection{MC MLE}

Utilité : retourne une estimation des paramètres selon la méthode décrite dans le papier de Geyer. 

```{r}
mc_mle = function(x, psi, h){
  
  m = length(x)

  y = hasting(x, m, psi, h)
    
  L = function(theta){
    return(sum(log(h(x,theta)/h(x,psi))) - m*log(mean(h(y,theta)/h(y,psi))))
  }
  
  theta = optim(
    par = rep(1,length(psi)),
    gr = "CG",
    control = list(fnscale=-1),
    fn = L
  )$par

  return(theta)
}
```


\subsection{NCE}

Utilité : Retourne l'estimation de la constante et des paramètres.

\begin{table}[!h]
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Argument} & \textbf{Type} & \textbf{Exemple} & \textbf{Indication} \\ \hline
x & vecteur & rcauchy(100, 0, 1) & notre échantillon de densité inconnue \\ \hline
law\_y & fonction & rnorm & fonction qui retourne un échantillon suivant la loi $p_n$ \\ \hline
n & entier & 100 & taille de l'échantillon de bruit suivant la loi $p_n$ \\ \hline
params\_y & vecteur & c(0,1) & arguments de la fonction law\_y \\ \hline
log\_pm & fonction &  & fonction qui retourne le logarithme de la densité $p_m$ \\ \hline
log\_pn & fonction &  & fonction qui retourne le logarithme de la densité $p_n$ \\ \hline
size\_theta & entier & 3 & taille de $\theta$, vaut habituellement 2 ou 3 \\ \hline
method & string & "CG" & méthode d'optimisation, habituellement "CG" ou "BFGS" \\ \hline
\end{tabular}
\end{table}

```{r}
nce = function(x, law_y, params_y, log_pm, log_pn, size_theta, n, methode = "CG"){
  
  y = do.call(law_y, c(list(n),params_y))
  
  m = length(x)
  
  h = function(u, theta){
    return( 1 / (1 + n/m * exp(log_pn(u) - log_pm(u, theta))))
  }
  
  J = function(theta){
    return( sum(log(h(x, theta))) + sum(log(1 - h(y, theta))) )
  }
  
  theta = optim(
    par = rep(1, size_theta),
    gr = methode,
    control = list(fnscale=-1),
    fn = J
  )$par
  
  return(c(theta[-size_theta], exp(-theta[size_theta])))
}
```

\subsection{Graphiques}

Utilité : afficher l'histogramme pour un échantillon de données $x$.

```{r}
print_hist = function(x) {
  df = data.frame(x = x)
  hist_x = ggplot(df, aes(x=x)) + geom_histogram(aes(y = stat(count) / sum(count)), bins = 20, color="white") + theme(aspect.ratio = 1) + labs(y = "Fréquence") + ggtitle("Distribution de l'échantillon x")
  print(hist_x)
}
```

Utilité : pour NCE, afficher l'évolution des paramètres au fur et à mesure de l'augmentation de n (la dimension de l'échantillon de bruit)

```{r}
NCE_evol_params = function(x, law_y, params_y, log_pm, log_pn, size_theta, ratio, steps, labels, methode = "CG") {
  
  # Creation de l'abscisse
  m = length(x)
  N = seq(0, m*ratio, length.out = steps + 1)
  
  # Creation de l'ordonnée
  theta = c()
  for (n in N) {
    theta = append(theta, nce(x, law_y, params_y, log_pm, log_pn, size_theta, n, methode))
  }
  
  # Formatage des données
  theta = t(rbind(matrix(theta, nrow = size_theta),N))
  df = as.data.frame(theta)
  df_melted = melt(df, id.vars = "N")
  
  # Plot
  plot_df = ggplot(df_melted, aes(x = N, y = value)) + 
  geom_line(aes(color = variable, group = variable)) +
  geom_point(aes(color = variable, group = variable)) +
  labs(title = "Evolution des paramètres par rapport au bruit", x = "n (taille du bruit)", y = "Paramètres", color = "Légende") +
  scale_color_manual(labels = labels, values = c("blue", "red", "orange"))
  
  print(plot_df)
  
  return(theta)
}
```

Note : il faudrait optimiser le temps de calcul de ces fonctions, peut-être en matriciel au lieu des boucles ou bien avec du calcul en parralèle sur CPU/GPU

\section{Exemple basique : la loi normale}

Soit $x$ l'échantillon de taille $m$ obtenu selon la loi de densité inconnue $p_d$.

On considère ici que $p_d$ appartient à la famille de fonctions paramétrées par $\theta = (c, \mu, \sigma)$ suivante :

$$p_m(u;\theta) = \frac{1}{Z(\mu, \sigma)} \times exp \big[-\frac{1}{2} \big(\frac{u - \mu}{\sigma} \big)^2 \big]  \quad \mbox{d'où} \quad ln(p_m(u;\theta)) = c - \frac{1}{2} \big(\frac{u}{\sigma} - \frac{\mu}{\sigma} \big)^2$$
```{r}
pm_barre = function(u, theta){
  return(exp(-0.5 * ((u - theta[1]) / theta[2]) ** 2))
}

log_pm = function(u,theta){
  return(theta[3] - 1/2 * (u/theta[2] - theta[1]/theta[2]) ** 2)
  # theta[1] = mu / theta[2] = sigma / theta[3] = c
}

log_pn_cauchy = function(u){
  return(log(dcauchy(u, mean(x), sd(x))))
}

m = 10000
n = 10000
x = rnorm(m, 2, 4)
size_theta = 3

# METHODE MC MLE
mc_mle(x, c(mean(x),sd(x)), pm_barre)

# METHODE GEYER
nce(x, rcauchy, c(mean(x),sd(x)), log_pm, log_pn_cauchy, size_theta, n)

```
```{r}
NCE_evol_params(x, rcauchy, c(mean(x),sd(x)), log_pm, log_pn_cauchy, size_theta, 10, 10, c("mu", "sigma", "exp(-c)"))
```


\section{Tentative de bootstrap}

Utilité : utilise le bootstrap sur x pour estimer les paramètres

```{r}
NCE_bootstrap = function(x, law_y, params_y, log_pm, log_pn, size_theta, n, nb_of_boot, labels, methode = "CG") {

  theta_bootstrap = c()
  for (i in 1:nb_of_boot) {
    x_bootstrap = sample(x, size = m, replace=TRUE)
    theta_bootstrap = append(theta_bootstrap, nce(x_bootstrap, law_y, params_y, log_pm, log_pn, size_theta, n, methode))
  }
  
  # Formatage des données
  theta_bootstrap = matrix(theta_bootstrap, nrow = size_theta)
  
  # Plot
  size_bootstrap = 1:nb_of_boot
  df = as.data.frame(cbind(t(rowCumsums(theta_bootstrap)) / size_bootstrap, size_bootstrap))
  df_melted = melt(df, id.vars = "size_bootstrap")
  
  plot_df = ggplot(df_melted, aes(x = size_bootstrap, y = value)) + 
  geom_line(aes(color = variable, group = variable)) +
  labs(title = "Evolution des paramètres par bootstrap", x = "Taille du bootstrap", y = "Paramètres", color = "Légende") +
  scale_color_manual(labels = labels, values = c("blue", "red", "orange"))
  
  print(plot_df)
  
  return(rowMeans(theta_bootstrap))
}
```

```{r}
(NCE_bootstrap(rnorm(1000,2,4), rcauchy, c(mean(x),sd(x)), log_pm, log_pn_cauchy, size_theta, 1000, 500, c("mu", "sigma", "exp(-c)")))
```


\section{Tentative récursivité}

Utilité : améliorer récursivement la précision de l'estimation via les estimations précédentes

```{r}
mc_mle_recursif = function(x, psi, h, size_of_loop){
  
  m = length(x)
  
  for (i in 1:size_of_loop) {
    y = hasting(x, m, psi, h)
      
    L = function(theta){
      return(sum(log(h(x,theta)/h(x,psi))) - m*log(mean(h(y,theta)/h(y,psi))))
    }
    
    psi = optim(
      par = rep(1,length(psi)),
      gr = "CG",
      control = list(fnscale=-1),
      fn = L
    )$par
    
    print(psi)
  }

  return(psi)
}
```

```{r}
mc_mle_recursif(x, c(mean(x),sd(x)), pm_barre, 10)
```
```{r}
mc_mle_recursif_2 = function(x, psi, h, size_of_loop){
  
  m = length(x)
  M = m
  
  for (i in 1:size_of_loop) {
    y = hasting(x, M, psi, h)
      
    L = function(theta){
      return(sum(log(h(x,theta)/h(x,psi))) - M*log(mean(h(y,theta)/h(y,psi))))
    }
    
    theta = optim(
      par = rep(1,length(psi)),
      gr = "CG",
      control = list(fnscale=-1),
      fn = L
    )$par
    
    x = append(x, hasting(x, m, theta, h))
    M = M + m
    
    print(theta)
  }

  return(theta)
}
```

```{r}
mc_mle_recursif_2(x, c(mean(x),sd(x)), pm_barre, 10)
```
```{r}
mc_mle_recursif_3 = function(x, psi, h, size_of_loop){
  
  m = length(x)
  M = m
  y = hasting(x, m, psi, h)
  
  for (i in 1:size_of_loop) {
      
    L = function(theta){
      return(sum(log(h(x,theta)/h(x,psi))) - M*log(mean(h(y,theta)/h(y,psi))))
    }
    
    theta = optim(
      par = rep(1,length(psi)),
      gr = "CG",
      control = list(fnscale=-1),
      fn = L
    )$par
    
    x = append(x, hasting(x, m, theta, h))
    y = append(y, hasting(x, m, psi, h))
    M = M + m
    
    print(theta)
  }

  return(theta)
}
```

```{r}
mc_mle_recursif_3(x, c(mean(x),sd(x)), pm_barre, 10)
```


```{r}
nce_recursif = function(x, law_y, params_y, log_pm, log_pn, size_theta, n, size_of_loop){
  
  y = do.call(law_y, c(list(n),params_y))
  
  m = length(x)
  
  for (i in 1:size_of_loop){
  
    h = function(u, theta){
      return( 1 / (1 + n/m * exp(log_pn(u) - log_pm(u, theta))))
    }
    
    J = function(theta){
      return( sum(log(h(x, theta))) + sum(log(1 - h(y, theta))) )
    }
    
    theta = optim(
      par = rep(1, size_theta),
      gr = "CG",
      control = list(fnscale=-1),
      fn = J
    )$par
    
    print(theta)
    
    y = do.call(law_y, c(list(n),theta[-size_theta]))
    
  }
  
  return(c(theta[-size_theta], exp(-theta[size_theta])))
}
```

```{r}
nce_recursif(x, rcauchy, c(mean(x),sd(x)), log_pm, log_pn_cauchy, size_theta, n, 10)
```

