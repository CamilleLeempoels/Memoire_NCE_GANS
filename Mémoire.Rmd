---
title: "Noise-contrastive estimation of normalising constants and GANs"
header-includes:
  - \usepackage{mathrsfs}
  - \usepackage{bm}
  - \usepackage{dsfont}
  - \usepackage{amsmath}
  - \usepackage{mdframed}
  - \usepackage{placeins}
output: 
  pdf_document:
    number_sections: true
---

\tableofcontents

\newpage
\section{Fonctions génériques}

```{r include=FALSE}
library(ggplot2)
library(reshape2)
library(matrixStats)
library(knitr)
library(moments) 
library(tseries)
library(readr)
library(Rcpp)

```

```{r include=FALSE}
# Palettes

pal5 = c("#3B9AB2", "#78B7C5", "#EBCC2A", "#DC863B", "#E1AF00")
pal4 = c("#3B9AB2", "#78B7C5", "#EBCC2A", "#DC863B")
pal2 = c("#3B9AB2", "#DC863B")
pal3 = c("#3B9AB2", "#EBCC2A", "#DC863B")

```


\subsection{Algorithme d'Hasting}

Utilité : simuler selon $p_m(.,\psi)$ pour un paramètre $\psi$ choisi. 

\begin{table}[!h]
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Argument} & \textbf{Type} & \textbf{Exemple} & \textbf{Indication} \\ \hline
x & vecteur & rcauchy(100, 0, 1) &  notre échantillon de densité inconnue \\ \hline
n & entier & 100 & taille de la simulation \\ \hline
psi & vecteur & c(0,1) & paramètres de la fonction h \\ \hline
h & fonction &  & fonction qui retourne $\overline{p_m}(.,\psi)$\\ \hline
\textbf{Sortie} & \textbf{Type} & \textbf{Exemple} & \textbf{Indication} \\ \hline
y & vecteur &  &  notre échantillon simulé \\ \hline
\end{tabular}
\end{table}

```{r hasting}
hasting = function(x, n, psi, h){
  y = c()
  y = append(y, sample(x, 1))
  for (i in 2:n){
    y_ = rnorm(1, y[i-1], 1)
    u = runif(1)
    if ( u <= 
         (h(y_,psi) * dnorm(y_, y[i-1], 1))
         /(h(y[i-1],psi) * dnorm(y[i-1], y_, 1))
    ){ 
      y = append(y, y_)} 
    else { 
      y = append(y, y[i-1])
    }
  }
  return (y)
}
```

Ci-dessous une autre version qui génère un échantillon iid.

\begin{table}[!h]
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Argument} & \textbf{Type} & \textbf{Exemple} & \textbf{Indication} \\ \hline
x & vecteur & rcauchy(100, 0, 1) &  notre échantillon de densité inconnue \\ \hline
n & entier & 100 & taille de la simulation \\ \hline
psi & vecteur & c(0,1) & paramètres de la fonction h \\ \hline
h & fonction &  & fonction qui retourne $\overline{p_m}(.,\psi)$\\ \hline
$\epsilon$ & Entier & 2 & pas de décorrélation $\overline{p_m}(.,\psi)$\\ \hline
\end{tabular}
\end{table}

```{r hasting iid}
hasting_iid = function(x, n, psi, h, eps){
  y = c()
  y = append(y, sample(x, 1))
  for (i in 2:(n*eps)){
    y_ = rnorm(1, y[i-1], 1)
    u = runif(1)
    if ( u <= 
         (h(y_,psi) * dnorm(y_, y[i-1], 1))
         /(h(y[i-1],psi) * dnorm(y[i-1], y_, 1))
    ){ 
      y = append(y, y_)} 
    else { 
      y = append(y, y[i-1])
    }
  }
  filter = y * rep(c(1,rep(0, eps-1)), n)
  return (filter[filter != 0])
}
```

\subsection{MC MLE (Geyer)}

Utilité : retourne une estimation des paramètres selon la méthode décrite dans le papier de Geyer. 

```{r mcmle}
mc_mle = function(x, n, psi, h){
  
  m = length(x)

  y = hasting(x, n, psi, h)
    
  L = function(theta){
    return(sum(log(h(x,theta)/h(x,psi))) - m*log(mean(h(y,theta)/h(y,psi))))
  }
  
  theta = optim(
    par = rep(1,length(psi)),
    gr = "CG",
    control = list(fnscale=-1),
    fn = L
  )$par

  return(theta)
}
```


\subsection{NCE (Gutmann)}

Utilité : Retourne l'estimation de la constante et des paramètres.

\begin{table}[!h]
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Argument} & \textbf{Type} & \textbf{Exemple} & \textbf{Indication} \\ \hline
x & vecteur & rcauchy(100, 0, 1) & notre échantillon de densité inconnue \\ \hline
law\_y & fonction & rnorm & fonction qui retourne un échantillon suivant la loi $p_n$ \\ \hline
n & entier & 100 & taille de l'échantillon de bruit suivant la loi $p_n$ \\ \hline
params\_y & vecteur & c(0,1) & arguments de la fonction law\_y \\ \hline
log\_pm & fonction &  & fonction qui retourne le logarithme de la densité $p_m$ \\ \hline
log\_pn & fonction &  & fonction qui retourne le logarithme de la densité $p_n$ \\ \hline
size\_theta & entier & 3 & taille de $\theta$, vaut habituellement 2 ou 3 \\ \hline
\end{tabular}
\end{table}

```{r nce}
nce = function(x, law_y, params_y, log_pm, log_pn, size_theta, n){
  
  y = do.call(law_y, c(list(n),params_y))
  
  m = length(x)
  
  h = function(u, theta){
    return( 1 / (1 + n/m * exp(log_pn(u) - log_pm(u, theta))))
  }
  
  J = function(theta){
    return( sum(log(h(x, theta))) + sum(log(1 - h(y, theta))) )
  }
  
  theta = optim(
    par = rep(1, size_theta),
    gr = "CG",
    control = list(fnscale=-1),
    fn = J
  )$par
  
  return(c(theta[-size_theta], exp(-theta[size_theta])))
}
```

\subsection{Graphiques}

Utilité : afficher l'histogramme pour un échantillon de données $x$.

```{r print_hist}
print_hist = function(x) {
  df = data.frame(x = x)
  hist_x = ggplot(df, aes(x=x)) + 
           geom_histogram(aes(y = ..density..), bins = 20, color="white", fill = "grey") + 
           labs(y = "Fréquence") +
           stat_function(fun = dcauchy, args = list(location = mean(df$x), scale = sd(df$x)), size = 1, aes(colour = "Cauchy")) +
           scale_colour_manual("Densité du bruit :", values=c(pal2[1]), labels = expression(Cauchy(m[emp],sd[emp]))) +
           theme(legend.position="bottom", legend.box="vertical", legend.margin=margin())
  print(hist_x)
}
```

Utilité : pour NCE, afficher l'évolution des paramètres au fur et à mesure de l'augmentation de n (la dimension de l'échantillon de bruit)

```{r NCE_evol_params}
NCE_evol_params = function(x, law_y, params_y, log_pm, log_pn, size_theta, ratio, steps, labels) {
  
  # Creation de l'abscisse
  m = length(x)
  N = seq(0, m*ratio, length.out = steps + 1)
  
  # Creation de l'ordonnée
  theta = c()
  for (n in N) {
    theta = append(theta, nce(x, law_y, params_y, log_pm, log_pn, size_theta, n))
  }
  
  # Formatage des données
  theta = t(rbind(matrix(theta, nrow = size_theta),N))
  df = as.data.frame(theta)
  df_melted = melt(df, id.vars = "N")
  
  # Plot
  plot_df = ggplot(df_melted, aes(x = N, y = value)) + 
  geom_line(aes(color = variable, group = variable)) +
  geom_point(aes(color = variable, group = variable)) +
  labs(title = "Evolution des paramètres par rapport au bruit", 
       x = "n (taille du bruit)", 
       y = "Paramètres", 
       color = "Légende") +
  scale_color_manual(labels = labels, values = c("blue", "red", "orange"))
  
  print(plot_df)
  
  #return(theta)
}
```

Note : il faudrait optimiser le temps de calcul de ces fonctions, peut-être en matriciel au lieu des boucles ou bien avec du calcul en parralèle sur CPU/GPU


\newpage

\section{Illustration avec la loi normale }

Soit $x$ l'échantillon de taille $m$ obtenu selon la loi de densité inconnue $p_d$.

On considère ici que $p_d$ appartient à la famille de fonctions paramétrées par $\theta = (c, \mu, \sigma)$ suivante :

$$p_m(u;\theta) = \frac{1}{Z(\mu, \sigma)} \times exp \big[-\frac{1}{2} \big(\frac{u - \mu}{\sigma} \big)^2 \big]  \quad \mbox{d'où} \quad ln(p_m(u;\theta)) = c - \frac{1}{2} \big(\frac{u}{\sigma} - \frac{\mu}{\sigma} \big)^2$$
```{r exemple1}
pm_barre = function(u, theta){
  return(exp(-0.5 * ((u - theta[1]) / theta[2]) ** 2))
}

log_pm = function(u,theta){
  return(theta[3] - 1/2 * (u/theta[2] - theta[1]/theta[2]) ** 2)
  # theta[1] = mu / theta[2] = sigma / theta[3] = c
}

log_pn_cauchy = function(u){
  return(log(dcauchy(u, mean(x), sd(x))))
}

m = 10000
n = 10000
x = rnorm(m, 2, 4)
size_theta = 3
```

\subsection{Méthode MC MLE}

```{r mc_mle application}
# METHODE MC MLE
mc_mle(x, n, c(mean(x),sd(x)), pm_barre)
```

Etudions l'impact de la dimension des échantillons sur la convergence des estimateurs.

```{r csv mc mle, eval=FALSE, include=FALSE}
#NE PAS FAIRE TOURNER CETTE CELLULE, HYPER LONG, IMPORTER LE CSV A LA PLACE

df_mcmle = data.frame(matrix(ncol = 4, nrow = 0))
colnames(df_mcmle_2) = c("param_1", "param_2", "size_data", "ratio_noise_data")

M = c(1000, 10000)


for (m in M){
  x = rnorm(m, 2, 4)
  psi = c(mean(x), sd(x))
  N = c(1, 10)
  for (n in N) {
    for (i in 1:100) {
       df_mcmle_2[nrow(df_mcmle_2) + 1, ] = c(mc_mle(x, m*n, psi, pm_barre), m, n)
    }
  }
}
```

```{r processing csv mc mle, include=FALSE}
df_mcmle <- read_csv("dataframes/df_mcmle.csv")[,-1]

df_mcmle$size_data = as.factor(df_mcmle$size_data)
df_mcmle$ratio_noise_data = as.factor(df_mcmle$ratio_noise_data)

df_mcmle_filt = subset(df_mcmle, param_2 <= 6 & param_2 >= 0 & param_1 >= -2 & param_1 <= 6)
```

```{r graph mc mle cv}
#png('df_mcmle.png', units="cm", width=15, height=11, res=300)

ggplot(df_mcmle_filt, aes(x = param_1, y = param_2, color = size_data, shape = ratio_noise_data)) + geom_point(size = 2) + scale_color_manual(values = pal2) + labs(shape="Ratio (n/m) : ", col="Taille de l'échantillon (m) : ") + xlab(expression(mu)) + ylab(expression(sigma)) + theme(legend.position="bottom", legend.box="vertical", legend.margin=margin()) + geom_vline(xintercept = 2, linetype="dotted") + geom_hline(yintercept = 4, linetype="dotted")

#dev.off()
```

Etudions l'impact du choix de $\psi$ sur la convergence des estimateurs.

```{r csv mc mle psi, eval=FALSE, include=FALSE}
df_mcmle_psi = data.frame(matrix(ncol = 3, nrow = 0))
colnames(df_mcmle_psi) = c("param_1", "param_2", "ratio_alpha_psi")

x = rnorm(10000, 2, 4)
for (r in c(1,3,5,7)){
  for (i in 1:100) {
     df_mcmle_psi[nrow(df_mcmle_psi) + 1, ] = c(mc_mle(x, 100000, c(2*r, 4*r), pm_barre), r)
  }
}
write.csv(df_mcmle_psi, "dataframes/df_mcmle_psi.csv")

```


```{r graph mc mle psi, message = FALSE}
df_mcmle_psi <- read_csv("dataframes/df_mcmle_psi.csv")[,-1]

df_mcmle_psi = df_mcmle_psi[order(-df_mcmle_psi$ratio_alpha_psi),]

lab = list(bquote(psi == "(2,4)"), bquote(psi == "(6,12)"), bquote(psi == "(10,20)"), bquote(psi == "(14,28)"))

df_mcmle_psi$ratio_alpha_psi = as.factor(df_mcmle_psi$ratio_alpha_psi)

#png('df_mcmle_psi.png', units="cm", width=15, height=11, res=300)

ggplot(df_mcmle_psi, aes(x = param_1, y = param_2, color = ratio_alpha_psi)) + geom_point(size = 2) + scale_color_manual(values = pal4, labels = lab) + labs(col=" ") + xlab(expression(mu)) + ylab(expression(sigma)) + theme(legend.position="bottom", legend.box="vertical", legend.margin=margin()) + geom_vline(xintercept = 2, linetype="dotted") + geom_hline(yintercept = 4, linetype="dotted")

#dev.off()
```

\subsection{Méthode NCE}

```{r nce application}
# METHODE NCE
nce(x, rcauchy, c(mean(x),sd(x)), log_pm, log_pn_cauchy, size_theta, n)
```

```{r csv nce, eval=FALSE, include=FALSE}
#NE PAS FAIRE TOURNER CETTE CELLULE, HYPER LONG, IMPORTER LE CSV A LA PLACE

df_nce = data.frame(matrix(ncol = 5, nrow = 0))
colnames(df_nce) = c("param_1", "param_2", "const", "size_data", "ratio_noise_data")

M = c(1000, 10000, 100000)


for (m in M){
  x = rnorm(m, 2, 4)
  psi = c(mean(x), sd(x))
  N = c(1, 10)
  for (n in N) {
    for (i in 1:50) {
       df_nce[nrow(df_nce) + 1, ] = c(nce(x, rcauchy, psi, log_pm, log_pn_cauchy, size_theta, m*n), m, n)
    }
  }
}
write.csv(df_nce, "dataframes/df_nce.csv")
```

```{r process csv nce, include=FALSE}
df_nce <- read_csv("dataframes/df_nce.csv")[,-1]

df_nce$size_data = as.factor(df_nce$size_data)
df_nce$ratio_noise_data = as.factor(df_nce$ratio_noise_data)

df_nce$const_error = abs(df_nce$const - 4*sqrt(2*pi))
```

```{r graph nce params}
#png('df_nce.png', units="cm", width=15, height=11, res=300)

ggplot(df_nce, aes(x = param_1, y = param_2, color = size_data, shape = ratio_noise_data)) + geom_point(size = 2) + scale_color_manual(values = pal3) + labs(shape="Ratio (n/m) : ", col="Taille de l'échantillon (m) : ") + xlab(expression(mu)) + ylab(expression(sigma)) + theme(legend.position="bottom", legend.box="vertical", legend.margin=margin()) + geom_vline(xintercept = 2, linetype="dotted") + geom_hline(yintercept = 4, linetype="dotted")

#dev.off()
```

```{r graph nce const}
#png('df_nce_const.png', units="cm", width=15, height=11, res=300)

ggplot(df_nce, aes(x = size_data, y = const_error, color = ratio_noise_data)) + geom_boxplot() + scale_color_manual(values = pal2) + labs(col="Ratio (n/m) : ") + xlab("Taille de l'échantillon (m)") + ylab("Erreur")

#dev.off()
```


\newpage
\section{Nouvelles approches}
\subsection{Bootstrap}


```{r NCE_bootstrap}

# Calcul de {size_boot} estimateurs par bootstrap
NCE_bootstrap = function(x, law_y, params_y, log_pm, log_pn, size_theta, n, size_boot, labels) {
  m = length(x)
  theta_bootstrap = c()
  x_bootstrap = x
  for (i in 1:size_boot) {
    theta_bootstrap = append(theta_bootstrap, nce(x_bootstrap, 
                                                  law_y, 
                                                  params_y, 
                                                  log_pm, 
                                                  log_pn, 
                                                  size_theta, 
                                                  n))
    x_bootstrap = sample(x, size = m, replace=TRUE)
  }
  return(matrix(theta_bootstrap, nrow = size_theta))
}

# Plot la moyenne empirique des estimateurs bootstrap en fonction du nombre d'estimateurs
NCE_bootstrap_plot = function(matrix_theta_bootstrap) {
  
  # Formatage des données pour plot
  array_boot = 1:length(matrix_theta_bootstrap[1,])
  df = as.data.frame(cbind(t(rowCumsums(matrix_theta_bootstrap))/array_boot,array_boot))
  df_melted = melt(df, id.vars = "array_boot")
  
  # Plot
  plot_df = ggplot(df_melted, aes(x = array_boot, y = value)) + 
  geom_line(aes(color = variable, group = variable)) +
  labs(title = "Evolution des paramètres par bootstrap", 
       x = "Taille du bootstrap", 
       y = "Paramètres", 
       color = "Légende") +
  scale_color_manual(labels = labels, values = c("blue", "red", "orange"))
  print(plot_df)
}


# etude bootstrap de l'estimateur
bootstrap = function(matrix, alpha){
  return(data.frame(
    theta = matrix_theta_bootstrap[,1],
    biais = rowMeans(matrix_theta_bootstrap) - matrix_theta_bootstrap[,1],
    IC = rowQuantiles(matrix_theta_bootstrap, probs = c(alpha/2, 1-alpha/2))
  ))
}

```

```{r test_NCE_bootstrap}
x_test = rnorm(1000,2,4)

matrix_theta_bootstrap = NCE_bootstrap(x_test, rcauchy, c(mean(x_test),sd(x_test)), log_pm, log_pn_cauchy, 3, 1000, 10, c("mu", "sigma", "exp(-c)"))

kable(bootstrap(matrix_theta_bootstrap, 0.05))
```



\subsection{Récursivité}

Utilité : améliorer récursivement la précision de l'estimation via les estimations précédentes

```{r mc_mle_recursif, eval=FALSE, include=FALSE}
m = 10000
n = 100000
x = rnorm(m, 2, 4)
psi = c(20,20)

df_recurs_naif = data.frame(matrix(ncol = 3, nrow = 0))
colnames(df_recurs_naif) = c("iteration", "param_1", "param_2")

for (i in 1:30) {
  y = hasting(x, n, psi, pm_barre)

  L = function(theta){
    return(sum(log(pm_barre(x,theta)/pm_barre(x,psi))) - m*log(mean(pm_barre(y,theta)/pm_barre(y,psi))))
  }
  
  psi = optim(
    par = rep(1,length(psi)),
    gr = "CG",
    control = list(fnscale=-1),
    fn = L
  )$par
  
  df_recurs_naif[nrow(df_recurs_naif) + 1, ] = c(i, psi)
}

write.csv(df_recurs_naif, "dataframes/df_recurs_naif.csv")
```


```{r graph lines mc mle recur, message = FALSE}
df_recurs_naif <- read_csv("dataframes/df_recurs_naif.csv")[,-1]
colnames(df_recurs_naif) = c("iteration", "param_1", "param_2")

lab = list(bquote(mu), bquote(sigma))

#png('df_recurs_naif.png', units="cm", width=15, height=11, res=300)

df_recurs_naif_melted = melt(df_recurs_naif, id.vars = "iteration")
ggplot(df_recurs_naif_melted, aes(x = iteration, y = value)) + geom_line(aes(color = variable, group = variable)) + scale_color_manual(values = pal2, labels = lab) + labs(col="Paramètres : ") + ylab("Valeurs des paramètres") + xlab("Itérations") + geom_hline(yintercept = 2, linetype="dotted") + geom_hline(yintercept = 4, linetype="dotted")  + theme(legend.position="bottom", legend.box="vertical", legend.margin=margin())

#dev.off()
```
Cette approche naïve n'améliore pas la précision de notre estimation. 

Nouvelle approche à venir.


```{r mc_mle_recursif_3, eval=FALSE, include=FALSE}
#NE FONCTIONNE PAS ENCORE

mc_mle_recursif_3 = function(x, n, psi, h, size_of_loop){
  
  m = length(x)
  PSI = data.frame(matrix(ncol = 2, nrow = 0))
  colnames(PSI) = c("param_1", "param_2")
  denom_x = 1
  denom_y = 1
  Y = c(hasting(x, n, psi, h))
  PSI[nrow(PSI) + 1, ] = psi
  print(PSI)

  for (i in 1:size_of_loop) {
    print(PSI)
    print(denom_x)
    print(denom_y)

    L = function(theta){
      return(sum(log(denom_x * h(x,theta)/h(x,PSI[i,]))) - m*log(mean(denom_y * h(Y,theta)/h(Y,PSI[i,]))))
      #return(sum(log(h(x,theta)/h(x,psi))) - m*log(mean(h(y,theta)/h(y,psi))))
    }
    
    PSI[nrow(PSI) + 1, ] = optim(
      par = rep(1,length(psi)),
      gr = "CG",
      control = list(fnscale=-1),
      fn = L
    )$par
    
    Y = append(Y, hasting(x, n, PSI[i,], h))
    
    denom_x = 0
    for (k in 1:i) {denom_x = denom_x + h(x, PSI[k,])}
    denom_y = 0
    for (k in 1:i) {denom_y = denom_y + h(Y, PSI[k,])}
  }

  return(PSI)
}

mc_mle_recursif_3(x, n, psi, pm_barre, 10)
```

\subsection{Hasting iid}

Afin de pouvoir appliquer numériquement la méthode de Reverse logistic regression, on aurait besoin de savoir simuler un échantillon iid suivant une loi dont on ne connait pas la constante de normalisation. L'idée est d'utiliser l'algorithme d'Hasting et de ne conserver qu'un échantillon tous les $\epsilon$ pas. Le code est en haut de ce document. 

Etude de l'impact du choix du pas.

```{r eval=FALSE, include=FALSE}
df_hasting_iid = data.frame(matrix(ncol=7, nrow=0))
colnames(df_hasting_iid) = c("epsilon", "mean", "sd", "skew", "kurtosis", "time", "mean_autocor")
init = rnorm(1, 0, 1)
n = 10000

for (eps in 1:10){
  for (i in 1:20) 
  { 
    t1<-Sys.time()
    y = hasting_iid(init, n, c(0,1), pm_barre, eps)
    t2<-Sys.time()
    t = difftime(t2, t1)
    df_hasting_iid[nrow(df_hasting_iid) + 1, ] = c(eps, mean(y), sd(y), skewness(y), kurtosis(y), t, mean(acf(y, plot = FALSE)$acf))
  }
}

df_hasting_iid$mean = abs(df_hasting_iid$mean)
df_hasting_iid$sd = abs(df_hasting_iid$sd - 1)
df_hasting_iid$skew = abs(df_hasting_iid$skew)
df_hasting_iid$kurtosis = abs(df_hasting_iid$kurtosis - 3)

write.csv(df_hasting_iid, "dataframes/df_hasting_iid.csv")
```

```{r include=FALSE}
df_hasting_iid <- read_csv("dataframes/df_hasting_iid.csv")[,-1]

df_hasting_iid_agg = aggregate(
  cbind(
    df_hasting_iid$mean, 
    df_hasting_iid$sd,
    df_hasting_iid$skew,
    df_hasting_iid$kurtosis,
    df_hasting_iid$time,
    df_hasting_iid$mean_autocor
  ),
  list(df_hasting_iid$epsilon),
  mean
)

colnames(df_hasting_iid_agg) = c("epsilon", "mean", "sd", "skew", "kurtosis", "time", "mean_autocor")

df_hasting_iid_melted = melt(df_hasting_iid_agg[,-6], id.vars = "epsilon")
```

```{r}
ggplot(df_hasting_iid_melted, aes(x = epsilon, y = value)) + geom_line(aes(color = variable, group = variable)) + scale_color_manual(values = pal5)

#ggplot(df_hasting_iid, aes(x = epsilon, y = time)) + geom_line()
```
$\epsilon = 2$ semble être un bon choix au regard du gain en terme d'auto-corrélation et du temps de calcul.

\subsection{Reverse logistic regression : deux lois de même famille}

La maximisation de la fonction objectif $$l_n(\eta) = \sum_{j=1}^m \sum_{i=1}^{n_j} log(p_j(X_{i,j}, \eta))$$ permet d'estimer les $\eta$ (qui sont fonction des constantes de normalisation des $h_j$). On utilise les notations suivantes :

$$\eta_j = -log(Z_j) + log(\frac{n_j}{n}) ~\mbox{ avec } Z_j \mbox{ la constante de normalisation de } h_j$$
$$p_j(x) = \frac{h_j(x)e^{\eta_j}}{\sum_{k=1}^m h_k(x)e^{\eta_k}}$$
Exemple avec $m=2$, $n = n_1 + n_2 = 1000 + 1000$, et pour coller avec les méthodes différentes on va prendre $h_1$ la densité non normalisée d'une $\mathscr{N}(\alpha)$ dont on a estimé $\alpha$ par MC MLE et $h_2$ la densité non normalisée d'une $\mathscr{N}(\psi)$ avec $\psi$ qu'on choisit. 

```{r rev_log_reg}
rev_log_reg = function(x, alpha, n, psi, h, eps){
  
  m = length(x)
  y = hasting_iid(x, n, psi, h, eps)
  
  # calcul des probabilités p_j
  denom = function(sample, eta) {
    return(pm_barre(sample, alpha)*exp(eta[1]) + pm_barre(sample,psi)*exp(eta[2]))}
  p_1 = function(sample, eta){
    return (pm_barre(sample, alpha)*exp(eta[1]) / denom(sample, eta))}
  p_2 = function(sample, eta){
    return (pm_barre(sample, psi)*exp(eta[2]) / denom(sample, eta))}
  
  # fonction objectif
  L = function(eta) {
    return(sum(log(p_1(x, eta))) + sum(log(p_2(y, eta))))}
  
  # initialisation descente de gradient
  eta1 = -log(sd(x)*sqrt(2*pi)) + log(m/(m+n))
  eta2 = -log(sd(y)*sqrt(2*pi)) + log(n/(m+n))
  
  # optimisation
  const = optim(
    par = c(eta1,eta2),
    gr = "CG",
    control = list(fnscale=-1),
    fn = L
  )$par
  
  a = exp(-const[1] + log(m/(m+n)))
  return(a)
}
```

```{r rev_log_reg init}
pm_barre = function(u, theta){
  return(exp(-0.5 * ((u - theta[1]) / theta[2]) ** 2))
}

m = 1000
n = 10000
x = rnorm(m,2,4)
psi = c(mean(x), sd(x))

alpha = mc_mle(x, n, psi, pm_barre)
print(alpha)
```

```{r rev_log_reg ex1}
print(rev_log_reg(x, alpha, n, c(8,8), pm_barre, 2))
```

Etude de l'impact de la dimension, du ratio et des paramètres sur la convergence de la constante.

```{r eval=FALSE, include=FALSE}
df_rev_log_reg = data.frame(matrix(ncol = 4, nrow = 0))
colnames(df_rev_log_reg) = c("const", "size_data", "ratio_noise_data", "ratio_alpha_psi")

M = c(1000, 5000, 10000, 15000)
N = c(1, 10)
alpha = c(2,4)

for (m in M){
  x = rnorm(m, 2, 4)
  for (n in N) {
    for (r in 1:4) {
      for (i in 1:15) {
         df_rev_log_reg[nrow(df_rev_log_reg) + 1, ] = c(rev_log_reg(x, alpha, n*m, alpha*r, pm_barre, 2), m, n, r)
      }
    }
  }
}

write.csv(df_rev_log_reg, "dataframes/df_rev_log_reg.csv")
```

```{r message=FALSE}
df_rev_log_reg <- read_csv("dataframes/df_rev_log_reg.csv")[,-1]

df_rev_log_reg_agg = aggregate(const ~ size_data + ratio_noise_data + ratio_alpha_psi,
                    data = df_rev_log_reg,
                    FUN = mean)
df_rev_log_reg_agg$ratio_alpha_psi = as.factor(df_rev_log_reg_agg$ratio_alpha_psi)
df_rev_log_reg_agg$ratio_noise_data = as.factor(df_rev_log_reg_agg$ratio_noise_data)
df_rev_log_reg_agg$size_data = as.numeric(df_rev_log_reg_agg$size_data)
df_rev_log_reg_agg$const_error = abs(df_rev_log_reg_agg$const - 4*sqrt(2*pi))

#png('df_rev_log_reg.png', units="cm", width=15, height=11, res=300)

ggplot(df_rev_log_reg_agg, aes(x = size_data, y = const_error, color = ratio_alpha_psi, shape = ratio_noise_data)) + geom_line() + geom_point() + labs(shape="Ratio (n/m) : ", col="Choix du paramètre : ") + xlab("Taille de l'échantillon X") + ylab("Erreur moyenne") + theme(legend.position="bottom", legend.box="vertical", legend.margin=margin()) + scale_color_manual(values = pal4, labels = list(bquote(psi == "(2,4)"), bquote(psi == "(4,8)"), bquote(psi == "(6,12)"), bquote(psi == "(8,16)"))
)

#dev.off()
```



Une tentative de la régression logistique inverse sur données gaussiennes, qui fonctionne bien, insensible aux conditions initiales. 
```{r}

rev_log_reg_gaussien = function(n1,n2,n3){
  
  pm1 = function(u){return(exp(-0.5 * ((u)**2)))} #connu
  pm2 = function(u,theta){return(exp(-0.5 * ((u - theta[1]) / theta[2]) ** 2))} #inconnu normalisation
  pm3 = function(u,theta){return(exp(-0.5 * ((u - theta[1]) / theta[2]) ** 2))} #inconnu normalisation
  
  x1 = rnorm(n1)
  x2 = rnorm(n2,5,3) # les paramètres sont (5,3) - psi 
  x3 = rnorm(n3,-2,6) # chi
  n  = n1+n2+n3
  
  ## ATTENTION A BIEN MATCHER LES PARAMETRES PSI ET CHI AVEC LES TIRAGES X1,X2 ##
  psi = c(5,3)
  chi = c(-2,6) 
  
  # calcul des probabilités p_j
  denom = function(sample, eta) {return(pm1(sample)*exp(eta[1]) + pm2(sample,psi)*exp(eta[2]) + pm3(sample,chi)*exp(eta[3]))}
  
  p_1 = function(sample, eta){return (pm1(sample)*exp(eta[1]) / denom(sample, eta))}
  p_2 = function(sample, eta){return (pm2(sample, psi)*exp(eta[2]) / denom(sample, eta))}
  p_3 = function(sample, eta){return (pm3(sample, chi)*exp(eta[3]) / denom(sample, eta))}
  
  # fonction objectif
  L = function(eta){return(sum(log(p_1(x1, eta))) + sum(log(p_2(x2, eta))) + sum(log(p_3(x3, eta))))}
  
  # initialisation descente de gradient
  eta1 =  -log(sd(x1)*sqrt(2*pi)) + log(n1/n) +100
  eta2 =  -log(sd(x2)*sqrt(2*pi)) + log(n2/n) +34
  eta3 =  -log(sd(x3)*sqrt(2*pi)) + log(n3/n) -45
  
  # optimisation
  eta = optim(par = c(eta1,eta2,eta3),gr = "CG",control = list(fnscale=-1),fn = L)$par
  
  constante_additive = eta[1] + log(sqrt(2*pi)) - log(n1/n)
  a = exp(-eta[1] + log(n1/n) + constante_additive) #valeur témoin / vrai valeur connue = 1
  b = exp(-eta[2] + log(n2/n) + constante_additive) # normlisation de linconnu
  c = exp(-eta[3] + log(n3/n) + constante_additive) # normlisation de linconnu
  
  x = c(a,b,c,constante_additive)
  
  return (x)
}


rev_log_reg_gaussien(100000,100000,100000)
c(sqrt(2*pi*1),sqrt(2*pi*9),sqrt(2*pi*36)) # pour comparer a,b,c aux vraies constantes de normalisations
```

\subsection{Reverse logistic regression : deux lois de familles différentes}

On reprend les notations ci-dessus (notations du papier). Exemple avec $m=2$, $n = n_1 + n_2$, et pour coller avec les méthodes différentes on va prendre $h_1$ la densité non normalisée d'une $\mathscr{N}(\alpha)$ dont on a estimé $\alpha$ par MC MLE et $h_2$ la densité d'une loi usuelle, on en connait donc la constante de normalisation. On note $\psi$ le paramètre de cette loi usuelle. 
```{r}
rev_log_reg_with_noise = function(x, law_noise, n, alpha, psi, h1, h2){
  
  y = do.call(law_noise, c(list(n),psi))
  m = length(x)
  
  # calcul des probabilités p_j
  denom = function(sample, eta) {
    return(h1(sample, alpha)*exp(eta[1]) + h2(sample,psi)*exp(eta[2]))}
  p_1 = function(sample, eta){
    return (h1(sample, alpha)*exp(eta[1]) / denom(sample, eta))}
  p_2 = function(sample, eta){
    return (h2(sample, psi)*exp(eta[2]) / denom(sample, eta))}
  
  # fonction objectif
  L = function(eta) {
    return(sum(log(p_1(x, eta))) + sum(log(p_2(y, eta))))}
  
  # initialisation descente de gradient
  eta1 = -log(sd(x)*sqrt(2*pi)) + log(m/(m+n))
  eta2 = -log(sd(y)*sqrt(2*pi)) + log(n/(m+n))
  
  # optimisation
  const = optim(
    par = c(eta1,eta2),
    gr = "CG",
    control = list(fnscale=-1),
    fn = L
  )$par
  
  constante_additive = const[2] + log(1) - log(n/(n+m))
  
  b = exp(-const[2] + log(n/(m+n)) + constante_additive)
  a = exp(-const[1] + log(m/(m+n)) + constante_additive)
  
  return(a)
}
```

```{r rev_log_reg_noise init}
pm_barre = function(u, theta){
  return(exp(-0.5 * ((u - theta[1]) / theta[2]) ** 2))
}

pn_cauchy = function(u, psi){
  return(dcauchy(u, psi[1], psi[2]))
}

pn_norm = function(u, psi){
  return(dnorm(u, psi[1], psi[2]))
}

pn_unif = function(u, psi){
  return(dunif(u, psi[1], psi[2]))
}

m = 1000
n = 10000
x = rnorm(m,2,4)
psi = c(mean(x), sd(x))

#alpha = mc_mle(x, n, psi, pm_barre)
```

```{r}
rev_log_reg_with_noise(x, rcauchy, n, c(2,4), c(mean(x),sd(x)), pm_barre, pn_cauchy)
```

```{r eval=FALSE, include=FALSE}
df_rev_log_reg_noise = data.frame(matrix(ncol = 4, nrow = 0))
colnames(df_rev_log_reg_noise) = c("const", "size_data", "ratio_noise_data", "law_noise")

M = c(1000, 5000, 10000, 15000)
N = c(1, 10)
alpha = c(2,4)

for (m in M){
  x = rnorm(m, 2, 4)
  for (n in N) {
    for (i in 1:1000) {
       df_rev_log_reg_noise[nrow(df_rev_log_reg_noise) + 1, ] = c(rev_log_reg_with_noise(x, rcauchy, n*m, alpha, alpha, pm_barre, pn_cauchy), m, n, "cauchy")
    }
    for (i in 1:1000) {
       df_rev_log_reg_noise[nrow(df_rev_log_reg_noise) + 1, ] = c(rev_log_reg_with_noise(x, rnorm, n*m, alpha, alpha, pm_barre, pn_norm), m, n, "normale")
    }
    for (i in 1:1000) {
       df_rev_log_reg_noise[nrow(df_rev_log_reg_noise) + 1, ] = c(rev_log_reg_with_noise(x, runif, n*m, alpha, c(min(x),max(x)), pm_barre, pn_unif), m, n, "unif")
    }
  }
}

write.csv(df_rev_log_reg_noise, "dataframes/df_rev_log_reg_noise.csv")
```

```{r message = FALSE}
df_rev_log_reg_noise <- read_csv("dataframes/df_rev_log_reg_noise.csv")[,-1]

df_rev_log_reg_agg = aggregate(const ~ size_data + ratio_noise_data + law_noise,
                    data = df_rev_log_reg_noise,
                    FUN = mean)
df_rev_log_reg_agg$ratio_noise_data = as.factor(df_rev_log_reg_agg$ratio_noise_data)
df_rev_log_reg_agg$size_data = as.numeric(df_rev_log_reg_agg$size_data)
df_rev_log_reg_agg$const_error = abs(df_rev_log_reg_agg$const - 4*sqrt(2*pi))

#png('df_rev_log_reg_noise.png', units="cm", width=15, height=11, res=300)

ggplot(df_rev_log_reg_agg, aes(x = size_data, y = const_error, color = law_noise, shape = ratio_noise_data)) + geom_line() + geom_point() + scale_color_manual(values = pal3, labels = c(expression(Cauchy(m[emp],sd[emp])), expression(Normale(m[emp],sd[emp])), expression(Uniforme(X[min],X[max])))) + xlab("Taille de l'échantillon X") + ylab("Erreur moyenne") + theme(legend.position="bottom", legend.box="vertical", legend.margin=margin()) + labs(shape="Ratio (n/m) : ", col="Bruit : ")

#dev.off()
```



\newpage
\section{Application : Modèle d'Ising}
\subsection{Simulation en 1D (première approche)}

```{r eval=FALSE}
# temps estimé 1min n=500, iter=100k
library('isingLenzMC')

sim_ising_1D = function(beta, nb_spins, nb_config){
  config  = rep(1,n) 
  data = matrix(nrow = iter, ncol = n)
  data[1, ] = config
  for (k in 2:iter){
    data[k,] = isStep1D(beta, config, 1.0, 0.0, 1)$vec # tirage avec Metropolis
  }
  return(data)
}

beta    = 0.9             # paramètre de temperature 
n       = 500             # nombre de sites
config  = rep(1,n)        # generer une configuration à n sites
config1 = genConfig1D(n)  # generer une configuration à n sites aléatoirement

energie = c()
iter = 10000
data = matrix(nrow= iter, ncol = n)

for (k in 1:iter){
  # on tire successivement des configurations jusqu'à obtenir convergence du niveau d'énergie
  #ce que l'on observera graphiquement
  config = isStep1D(beta, config, 1.0, 0.0, 1)$vec # tirage avec Metropolis
  data[k,] = config
  energie   = c(energie,totalEnergy1D(config,1.0,0.0))
}
plot(energie, type ='l')
```


On définit la fonction $coeur$ qui est à une constante de normalisation près la mesure de Gibbs associée à au modèle d'Ising de paramètre $\beta$.
On définit la fonction $ising1D(n)$ qui est rend toutes les configurations de spins possibles d'un modèle d'Ising unidimensionnel à $n$ sites. 

```{r eval=FALSE}
coeur = function(constante_normalisation,beta=0.9,config){
  
  # constante_normalisation :  fonction de partition inconnue
  # beta                    :  paramètre de température 
  # config                  :  une configuration de spins
  
  # Lorsque 'constante_normalisation' == vraie valeur de la fonction de partition pour le paramètre beta 
  #       alors return la probabilité de la configuration de spins 'config'
  
  return ( exp(-beta*totalEnergy1D(config,1.0,0.0))/constante_normalisation )
}
  
ising1D = function(n){

  # n : nombre de sites pour une configuration
  
  # return l'ensemble des configuration 1D possibles dans une matrice
  # chaque ligne correspond à une configuration possible
  
  if (n==1)
    {
    return(matrix(c(1,-1),nrow=2))
    }
  else
    {
    return(cbind( rbind(ising1D(n-1),ising1D(n-1)), c(rep(1, 2**(n-1)), rep(-1,2**(n-1)))))
    }
}

```

```{r eval=FALSE}
# un petit test de ce qu'on fait

n       = 15   # nombre de sites 
config  = ising1D(n) # ensemble des configurations 1D à n sites
beta    = 0.9 # paramètre de température
energie = c()

for (k in 1:2**n){ energie = c(energie,totalEnergy1D(c(config[k,]), 1, 0) ) } # totalEnergy1D(config[k,], 1, 0), 1 interraction voisins, 0 champs ext

cst_normalisation  = sum(exp(-beta*energie))

proba  = 0
for (k in 1:2**n){ proba = proba + coeur(cst_normalisation,beta=0.9,config[k,]) }
(proba)
print("if proba = 1, then it is good")
```

Il se peut qu'il y ait conflit de notations pour cette sous section d'Ising avec les notations du précédent exemple.
De plus, il y'a une erreur en sortie que je n'arrive pas résoudre pour l'instant. Mais je ne pense pas qu'elle soit très compliquée...

```{r eval=FALSE}
log_pn = function(u,param_pn){
  # pn densite iid N(param_pn[1],param_pn[2]) de dim length(u) 
  # Verifier que c'est bien ce qui est codé
  n     = length(u)
  log_d = 0 
  for (k in 1:n){ log_d = log_d - 0.5 * ((u[k] - param_pn[1]) / param_pn[2]) ** 2 }
  return(-0.5*n*log(2*pi*(param_pn[2]**2)) + log_d)
}

log_pm = function(configuration,beta,theta){
  # theta = -log(Z)
  # interraction entre sites = 1 et avec le champs extérieure = 0
  return ( -beta*totalEnergy1D(configuration,1,0) + theta )
}


nce_ising = function(matrix_ising,beta,log_pm,theta,law_pn,param_pn,log_pn,b_size){
  # matrix_ising = matrice ou en ligne sont rangées les observations d'Ising en 1D
  # beta         = paramètre de température de modèle d'Ising \in ]0,1[
  # log_pm       = logarithme de la densité d'Ising
  # theta        = paramètre à optimiser , en lien avec la constante de normalisation
  
  # law_pn   = loi du bruit qui doit contenir dans son support {0,1}
  # param_pn = paramètres de la loi law_pn
  # log_pn   = log de la densité de pn
  
  # b_size  = nombre d'observation du bruit, à ne pas confondre avec la taille d'une observation du bruit qui est fixée à n
  
  n     = dim(matrix_ising)[2] # n = nombre de sites pour les observations d'Ising en 1D
  m     = dim(matrix_ising)[1] # m = nombre d'observations d'Ising en 1D
  
  bruit = matrix(do.call(law_pn,c(list(b_size*n),param_pn)),ncol=n) # échantillon de vecteurs de bruit disposé en matrice, ligne par ligne
  
  h = function(configuration,theta){return( 1 / (1 + b_size/m * exp(log_pn(configuration,param_pn) - log_pm(configuration,beta,theta))))}
  
  J = function(theta){
    # x vecteur de densite inconnue 
    # y vecyeur de bruit 
    objectif = 0
    for (k in 1:m){objectif = objectif + log(h(matrix_ising[k,],theta)) }   # composante de l'objectif liée aux données
    for (k in 1:b_size){objectif = objectif + log(1 - h(bruit[k,],theta)) } # composante de l'objectif liée au bruit
    return( objectif )
    }
  
  solution = optimize(f = J,
                      interval = c(-1e5,1e5),
                      maximum  = FALSE)
  
  return(exp(-solution))
}


n            = 10
matrix_ising = ising1D(n)[100:900,]
beta         = 0.9
theta        = 1
law_pn       = rnorm
param_pn     = c(0,1)
b_size       = 2**n

nce_ising(matrix_ising,beta,log_pm,theta,law_pn,param_pn,log_pn,b_size)
```

\subsection{Simulation en 1D (deuxième approche)}


```{r eval=FALSE}
# --------------- Ising ---------------------------------------------------------------------------------------------------------

all_config_ising1D = function(nb_spins){
  # n : nombre de sites pour une configuration
  # return l'ensemble des configuration 1D possibles dans une matrice
  # chaque ligne correspond à une configuration possible
  
  if (nb_spins == 1) {
    return(matrix(c(1,-1),nrow=2))
  } else {
    return(cbind(rbind(all_config_ising1D(nb_spins - 1), all_config_ising1D(nb_spins - 1)), c(rep(1,2**(nb_spins - 1)),rep(-1,2**(nb_spins - 1)))))
  }
}

sim_ising_1D = function(beta, nb_config, nb_spins){
  config  = rep(1,nb_spins) 
  data = matrix(nrow = nb_config, ncol = nb_spins)
  data[1, ] = config
  data_for_plot = c(config)
  for (k in 2:nb_config){
    data[k,] = isStep1D(beta, data[k-1,], 1.0, 0.0, 1)$vec # tirage avec Metropolis
    data_for_plot = append(data_for_plot, data[k,])
  }
  return(data)
}

graph_ising = function(configs){
  shape = dim(configs)
  nb_config = shape[1]
  nb_spins = shape[2]
  
  valeur_spin = c(t(configs))
  iterations = rep(1:nb_config, rep(nb_spins, nb_config))
  id_site = rep(1:nb_spins, nb_config)
  
  df = data.frame(valeur_spin, iterations, id_site)
  return(ggplot(df, aes(iterations, id_site, fill = valeur_spin)) + geom_tile())
}

calc_energie = function(sim_ising, nb_config){
  sim_ising_split = split(sim_ising, as.factor(1:nb_config))
  energie = sapply(sim_ising_split, totalEnergy1D, 1, 0)   # interaction entre sites = 1 et avec le champ extérieur = 0
  return(energie)
}

calc_const = function(beta,nb_spins){
  ens_config = all_config_ising1D(nb_spins) # ensemble des configurations 1D à nb_spins sites
  energie_total = calc_energie(ens_config, 2**nb_spins)
  return(sum(exp(-beta*energie_total)))
}

# --------------- Simulations d'échantillons ------------------------------------------------------------------------------------

sim_multivar_dis_unif = function(nb_samples, size_sample, a, b){
  return(matrix(sample(c(a,b), nb_samples * size_sample, replace=T), ncol = size_sample))
}

sim_multivar_cont_unif = function(nb_samples, size_sample, minimum, maximum){
  return(matrix(runif(nb_samples * size_sample, minimum, maximum), ncol = size_sample))
}


# --------------- Fonctions pm_barre, pm et pn -----------------------------------------------------------------------------------

log_pm_ising = function(u,theta){
  # theta[1] = beta
  # theta[2] = -log(Z)
  return ( - theta[1] * calc_energie(u, dim(u)[1]) + theta[2] )
}

pn_multivar_cont_unif = function(u, psi){
  return(rowProds(dunif(u, -1, 1)))
}

pn_multivar_dis_unif = function(u, psi){
  return(rowProds(ifelse(u == -1, 1/2, ifelse(u == 1, 1, 0))))
}

log_pn_multivar_cont_unif = function(u){
  return(log(rowProds(dunif(u, -1, 1))))
}

log_pn_multivar_dis_unif = function(u){
  return(log(rowProds(ifelse(u == -1, 1/2, ifelse(u == 1, 1, 0)))))
}

pm_barre_ising = function(u, theta){
  return(exp(-theta[1] * calc_energie(u, dim(u)[1])))
}

```


```{r eval=FALSE}

# ------------- Variables globales -----------------------------------------------------------------------------------------------

beta_alpha = 0.5
beta_psi = 0.1
nb_spins = 15
nb_config = 100000
x = sim_ising_1D(beta_alpha, nb_config, nb_spins)
y = sim_ising_1D(beta_psi, nb_config, nb_spins)

#energie_x = calc_energie(x, nb_config)
#energie_y = calc_energie(y, nb_config)

graph_ising(x)
graph_ising(y)

# --------------- Calcul numérique de la constante --------------------------------------------------------------------------

#calc_const(beta_alpha, nb_spins)

# --------------- Estimation avec MC MLE ------------------------------------------------------------------------------------


#L = function(theta){ return(sum(log(pm_barre_ising(x,theta)/pm_barre_ising(x,psi))) - m*log(mean(pm_barre_ising(y,theta)/pm_barre_ising(y,psi))))}

#theta = optim( par = c(0.9, 1), gr = 'CG', control = list(fnscale=-1), fn = L)$par


# --------------- Estimation avec la regression logistique inverse ------------------------------------------------------

rev_log_reg_with_noise(x, sim_multivar_cont_unif, nb_config, c(beta_alpha), list(nb_spins, -1, 1), pm_barre_ising, pn_multivar_cont_unif)


# --------------- Estimation avec NCE -----------------------------------------------------------------------------------

#nce(x, sim_multivar_dis_unif, list(nb_spins,-1,1), log_pm_ising, log_pn_multivar_dis_unif, 2, n)

```

\subsection{Simulation en 2D}

Algorithme de Gibbs : on part d'une configuration aléatoire. A chaque itération, on sélectionne un site $\sigma_j$ au hasard. Ce site prend la valeur 1 avec probabilité $p := \mathbb{P}(\sigma_j = 1 | \sigma_{-j})$ et -1 avec probabilité $1-p$. $\sigma_{-j}$ est une notation qui désigne les voisins de $\sigma_j$.

On a $p = \frac{1}{1 + exp(\beta \Delta H)}$ avec $\Delta H = 2\sigma_j\sum_j\sigma_{-j}$.

```{r}
# Initialisation de façon uniforme
init_ising = function(heigth, width){
  return(matrix(sample(c(-1,1), heigth * width, replace=T), ncol = width))
}

# Sélection des voisins, dans l'ordre haut - droite - bas - gauche

get_neighbors = function(sample, row_site, col_site){
  height = dim(sample)[1]
  width = dim(sample)[2]

  if (height == 1){
    up = 0
    down = 0
  }
  else if (row_site == 1){
    up = 0
    down = sample[row_site + 1,col_site]
  } 
  else if (row_site == height) {
    up = sample[row_site - 1,col_site]
    down = 0
  }
  else {
    up = sample[row_site - 1,col_site]
    down = sample[row_site + 1,col_site]
  }
  if (width == 1){
    left = 0
    right = 0
  }
  else if (col_site == 1){
    left = 0
    right = sample[row_site, col_site + 1]
  } 
  else if (col_site == width) {
    left = sample[row_site, col_site - 1]
    right = 0
  }
  else {
    left = sample[row_site, col_site - 1]
    right = sample[row_site, col_site + 1]
  }
  
  return(c(up, right, down, left))
}

# Calcul de l'énergie

compute_energy = function(sample){
  energy = 0
  height = dim(sample)[1]
  width = dim(sample)[2]
  for (i in 1:height){
    for (j in 1:width){
      energy = energy + sample[i,j]*sum(get_neighbors(sample, i, j))
    }
  }
  return(energy/2)
}
  
compute_delta_energie = function(sample, row_site, col_site){
  spin = sample[row_site, col_site]
  return(2 * spin * sum(get_neighbors(sample, row_site, col_site)))
}

# Simulation

sim_ising_2D = function(beta, heigth, width, iter, init) {
  config = init
  for (i in 1:iter) {
    row = sample(1:heigth, 1)
    col = sample(1:width, 1)
    p = 1/(1 + exp(beta * compute_delta_energie(config, row, col)))
    config[row, col] = sample(c(1,-1), 1, prob = c(p, 1-p))
  }
  return(config)
}

# Graphique 

graph_config = function(config){
  height = dim(config)[1]
  width = dim(config)[2]
  colnames(config) <- paste("Col", 1:width)
  rownames(config) <- paste("Row", 1:height)
  df <- melt(config)
  colnames(df) <- c("x", "y", "value")
  
  return(ggplot(df, aes(x = x, y = y, fill = value))
         + geom_tile() 
         + coord_fixed()
         + theme(axis.ticks.x = element_blank(), axis.text.x = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank()))
}
```

On va générer un échantillon de $m$ configurations distribuées selon un modèle d'Ising en 2D. Le résultat sera présenté sous la forme d'une matrice verticale où seront empilées les configurations.

```{r}
samples_ising_2D = function(beta, heigth, width, iter, epsilon, m){
  # beta : paramètre du modèle
  # heigth, width : taille d'une configuration
  # iter : nombre d'itérations pour atteindre la stationnarité
  # epsilon : pas de décorrélation pour sélectionner des configs iid
  # m : nombre de configurations dans notre échantillon
  
  # initialisation
  current_config = init_ising(heigth, width)
  
  # On effectue des itérations jusqu'à atteindre supposément l'état stationnaire
  current_config = sim_ising_2D(beta, heigth, width, iter, current_config)
  samples = current_config
  
  # Une fois la stationnarité atteinte, on prend un échantillon tous les epsilon pas afin que nos échantillons soient iid
  for (i in 2:m){
    current_config = sim_ising_2D(beta, heigth, width, epsilon, current_config)
    samples = rbind(samples, current_config)
  }
  
  return(samples)
}

```

```{r}
beta = 0.5
width = 15
heigth = 15
iter = 100000
epsilon = 100
m = 10000
init = init_ising(heigth, width)

test = sim_ising_2D(beta, heigth, width, iter, init)
#graph_config(test)

samples = samples_ising_2D(beta, heigth, width, iter, epsilon, m)
```

```{r}
graph_config(samples[1:15,])
graph_config(samples[149986:150000,])
```

